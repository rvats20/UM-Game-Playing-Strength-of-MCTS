{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":70089,"databundleVersionId":9515283,"sourceType":"competition"},{"sourceId":7074842,"sourceType":"datasetVersion","datasetId":4074593},{"sourceId":7570020,"sourceType":"datasetVersion","datasetId":4021289},{"sourceId":9341631,"sourceType":"datasetVersion","datasetId":5661348},{"sourceId":203746103,"sourceType":"kernelVersion"}],"dockerImageVersionId":30776,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Based on single LightGBM baseline with minimal FE, LB **0.447**:\n\nhttps://www.kaggle.com/code/snufkin77/mcts-strength-relevant-baseline\n\nDeepTables: Deep-learning Toolkit for Tabular data\n\nhttps://github.com/DataCanvasIO/DeepTables\n\nhttps://deeptables.readthedocs.io/en/latest/model_config.html#parameters\n\n**Version 1**: single DeepTables NN baseline, LB **0.462**.\n\n**Version 6**: single DeepTables NN, LB **0.448**; `ModelConfig(apply_gbm_features=True)`.\n\n**Version 7**: single DeepTables NN, LB **0.438**; `ModelConfig(apply_gbm_features=True)`, `ModelConfig(nets=['dnn_nets'] + ['fm_nets'] + ['cin_nets'])`.\n\n**Version 8**: same as Version 7 + scaling all numerical features to fix issue with divergence in the validation scores. Now overall CV rmse (0.4324) is closer to LB **0.435**. Note that in this version, the `fit_transform` method of scaler was performed on train and test data combined, which is not a good practice, although it does not violate any Kaggle rules.\n\n**Version 9**: same as Version 8, but using scaler with `fit_transform` on the training data alone and then applying `transform` to the test data. CV 0.4319 | LB **0.438**.\n\n**Version 10**: scaling way from Version 9, enable `LearningRateScheduler` with warmup `(LR_START = 1e-4)` on first epoch. CV 0.4343 | LB **0.434**.\n\n**Version 11**: single CatBoost baseline. CV 0.4160 | LB 0.434.\n\n**Version 12**: ensemble NN (CV 0.4343 | LB 0.434) + CatBoost (CV 0.4160 | LB 0.434), `ens_weights = {'nn': 0.50, 'ctb': 0.50}`. CV 0.4138 | LB **0.430**.\n\n**Version 14**: adjust CatBoost params and ensemble weights; ensemble NN (CV 0.4343 | LB 0.434) + CatBoost (CV 0.4154 | LB 0.431), `ens_weights = {'nn': 0.40, 'ctb': 0.60}`. CV 0.4116 | LB **0.428**.\n\n**Version 16**: same as Version 14, but due to non-deterministic behavior during training on GPU, the final score may slightly variate around. Based on this [contribution](https://www.kaggle.com/competitions/um-game-playing-strength-of-mcts-variants/discussion/542932), the DeepTables model saving-loading issue has been overcome. Now `Output` contains saved pretrained models that can be used for inference.","metadata":{}},{"cell_type":"code","source":"!pip install --no-index -U --find-links=/kaggle/input/tensorflow-2-15/tensorflow tensorflow==2.15.0\n!pip install --no-index -U --find-links=/kaggle/input/deeptables-v0-2-5/deeptables-0.2.5 deeptables==0.2.5\n!pip install --no-index -U --find-links=/kaggle/input/fix-deeptables/deeptables-0.2.6 deeptables==0.2.6","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-11-28T16:56:34.116319Z","iopub.execute_input":"2024-11-28T16:56:34.116880Z","iopub.status.idle":"2024-11-28T16:57:52.356835Z","shell.execute_reply.started":"2024-11-28T16:56:34.116837Z","shell.execute_reply":"2024-11-28T16:57:52.355643Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Looking in links: /kaggle/input/tensorflow-2-15/tensorflow\nProcessing /kaggle/input/tensorflow-2-15/tensorflow/tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (24.3.25)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (0.5.4)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (0.2.0)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (3.11.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (18.1.1)\nProcessing /kaggle/input/tensorflow-2-15/tensorflow/ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from tensorflow==2.15.0)\nRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (1.26.4)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (70.0.0)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (1.16.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (2.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (4.12.2)\nProcessing /kaggle/input/tensorflow-2-15/tensorflow/wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from tensorflow==2.15.0)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (0.37.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (1.62.2)\nProcessing /kaggle/input/tensorflow-2-15/tensorflow/tensorboard-2.15.1-py3-none-any.whl (from tensorflow==2.15.0)\nRequirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (2.15.0)\nProcessing /kaggle/input/tensorflow-2-15/tensorflow/keras-2.15.0-py3-none-any.whl (from tensorflow==2.15.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.43.0)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.30.0)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.6)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.32.3)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow==2.15.0) (3.1.2)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.0.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2024.8.30)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.1.5)\nRequirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.0)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\nInstalling collected packages: wrapt, ml-dtypes, keras, tensorboard, tensorflow\n  Attempting uninstall: wrapt\n    Found existing installation: wrapt 1.16.0\n    Uninstalling wrapt-1.16.0:\n      Successfully uninstalled wrapt-1.16.0\n  Attempting uninstall: ml-dtypes\n    Found existing installation: ml-dtypes 0.3.2\n    Uninstalling ml-dtypes-0.3.2:\n      Successfully uninstalled ml-dtypes-0.3.2\n  Attempting uninstall: keras\n    Found existing installation: keras 3.3.3\n    Uninstalling keras-3.3.3:\n      Successfully uninstalled keras-3.3.3\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.16.2\n    Uninstalling tensorboard-2.16.2:\n      Successfully uninstalled tensorboard-2.16.2\n  Attempting uninstall: tensorflow\n    Found existing installation: tensorflow 2.16.1\n    Uninstalling tensorflow-2.16.1:\n      Successfully uninstalled tensorflow-2.16.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.9.1 requires tensorflow~=2.16.1, but you have tensorflow 2.15.0 which is incompatible.\ntensorflow-serving-api 2.16.1 requires tensorflow<3,>=2.16.1, but you have tensorflow 2.15.0 which is incompatible.\ntensorflow-text 2.16.1 requires tensorflow<2.17,>=2.16.1; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.15.0 which is incompatible.\ntensorstore 0.1.65 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\ntf-keras 2.16.0 requires tensorflow<2.17,>=2.16, but you have tensorflow 2.15.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed keras-2.15.0 ml-dtypes-0.2.0 tensorboard-2.15.1 tensorflow-2.15.0 wrapt-1.14.1\nLooking in links: /kaggle/input/deeptables-v0-2-5/deeptables-0.2.5\nProcessing /kaggle/input/deeptables-v0-2-5/deeptables-0.2.5/deeptables-0.2.5-py3-none-any.whl\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from deeptables==0.2.5) (21.3)\nRequirement already satisfied: scipy>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from deeptables==0.2.5) (1.14.1)\nRequirement already satisfied: pandas>=0.25.3 in /opt/conda/lib/python3.10/site-packages (from deeptables==0.2.5) (2.2.2)\nRequirement already satisfied: numpy>=1.16.5 in /opt/conda/lib/python3.10/site-packages (from deeptables==0.2.5) (1.26.4)\nRequirement already satisfied: scikit-learn>=0.22.1 in /opt/conda/lib/python3.10/site-packages (from deeptables==0.2.5) (1.2.2)\nRequirement already satisfied: lightgbm>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from deeptables==0.2.5) (4.2.0)\nRequirement already satisfied: category-encoders>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from deeptables==0.2.5) (2.6.3)\nProcessing /kaggle/input/deeptables-v0-2-5/deeptables-0.2.5/hypernets-0.3.1-py3-none-any.whl (from deeptables==0.2.5)\nRequirement already satisfied: h5py>=2.10.0 in /opt/conda/lib/python3.10/site-packages (from deeptables==0.2.5) (3.11.0)\nRequirement already satisfied: eli5 in /opt/conda/lib/python3.10/site-packages (from deeptables==0.2.5) (0.13.0)\nRequirement already satisfied: statsmodels>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from category-encoders>=2.1.0->deeptables==0.2.5) (0.14.2)\nRequirement already satisfied: patsy>=0.5.1 in /opt/conda/lib/python3.10/site-packages (from category-encoders>=2.1.0->deeptables==0.2.5) (0.5.6)\nRequirement already satisfied: fsspec>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from hypernets>=0.2.5.1->deeptables==0.2.5) (2024.6.1)\nRequirement already satisfied: ipython in /opt/conda/lib/python3.10/site-packages (from hypernets>=0.2.5.1->deeptables==0.2.5) (8.21.0)\nRequirement already satisfied: traitlets in /opt/conda/lib/python3.10/site-packages (from hypernets>=0.2.5.1->deeptables==0.2.5) (5.14.3)\nProcessing /kaggle/input/deeptables-v0-2-5/deeptables-0.2.5/XlsxWriter-3.1.9-py3-none-any.whl (from hypernets>=0.2.5.1->deeptables==0.2.5)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from hypernets>=0.2.5.1->deeptables==0.2.5) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from hypernets>=0.2.5.1->deeptables==0.2.5) (6.0.2)\nProcessing /kaggle/input/deeptables-v0-2-5/deeptables-0.2.5/paramiko-3.4.0-py3-none-any.whl (from hypernets>=0.2.5.1->deeptables==0.2.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from hypernets>=0.2.5.1->deeptables==0.2.5) (2.32.3)\nRequirement already satisfied: tornado in /opt/conda/lib/python3.10/site-packages (from hypernets>=0.2.5.1->deeptables==0.2.5) (6.4.1)\nRequirement already satisfied: prettytable in /opt/conda/lib/python3.10/site-packages (from hypernets>=0.2.5.1->deeptables==0.2.5) (3.10.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from hypernets>=0.2.5.1->deeptables==0.2.5) (4.66.4)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from hypernets>=0.2.5.1->deeptables==0.2.5) (1.4.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.25.3->deeptables==0.2.5) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.25.3->deeptables==0.2.5) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.25.3->deeptables==0.2.5) (2024.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.22.1->deeptables==0.2.5) (3.5.0)\nRequirement already satisfied: attrs>17.1.0 in /opt/conda/lib/python3.10/site-packages (from eli5->deeptables==0.2.5) (23.2.0)\nRequirement already satisfied: jinja2>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from eli5->deeptables==0.2.5) (3.1.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from eli5->deeptables==0.2.5) (1.16.0)\nRequirement already satisfied: graphviz in /opt/conda/lib/python3.10/site-packages (from eli5->deeptables==0.2.5) (0.20.3)\nRequirement already satisfied: tabulate>=0.7.7 in /opt/conda/lib/python3.10/site-packages (from eli5->deeptables==0.2.5) (0.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->deeptables==0.2.5) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=3.0.0->eli5->deeptables==0.2.5) (2.1.5)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (5.1.1)\nRequirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (0.19.1)\nRequirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.10/site-packages (from ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (0.1.7)\nRequirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.10/site-packages (from ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (3.0.47)\nRequirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (2.18.0)\nRequirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (0.6.2)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (1.2.0)\nRequirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (4.9.0)\nProcessing /kaggle/input/deeptables-v0-2-5/deeptables-0.2.5/bcrypt-4.1.2-cp39-abi3-manylinux_2_28_x86_64.whl (from paramiko->hypernets>=0.2.5.1->deeptables==0.2.5)\nRequirement already satisfied: cryptography>=3.3 in /opt/conda/lib/python3.10/site-packages (from paramiko->hypernets>=0.2.5.1->deeptables==0.2.5) (42.0.8)\nProcessing /kaggle/input/deeptables-v0-2-5/deeptables-0.2.5/PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (from paramiko->hypernets>=0.2.5.1->deeptables==0.2.5)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prettytable->hypernets>=0.2.5.1->deeptables==0.2.5) (0.2.13)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->hypernets>=0.2.5.1->deeptables==0.2.5) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->hypernets>=0.2.5.1->deeptables==0.2.5) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->hypernets>=0.2.5.1->deeptables==0.2.5) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->hypernets>=0.2.5.1->deeptables==0.2.5) (2024.8.30)\nRequirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography>=3.3->paramiko->hypernets>=0.2.5.1->deeptables==0.2.5) (1.16.0)\nRequirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (0.8.4)\nRequirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (0.7.0)\nRequirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (2.0.1)\nRequirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (2.4.1)\nRequirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (0.2.2)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=3.3->paramiko->hypernets>=0.2.5.1->deeptables==0.2.5) (2.22)\nInstalling collected packages: XlsxWriter, bcrypt, pynacl, paramiko, hypernets, deeptables\nSuccessfully installed XlsxWriter-3.1.9 bcrypt-4.1.2 deeptables-0.2.5 hypernets-0.3.1 paramiko-3.4.0 pynacl-1.5.0\nLooking in links: /kaggle/input/fix-deeptables/deeptables-0.2.6\nProcessing /kaggle/input/fix-deeptables/deeptables-0.2.6/deeptables-0.2.6-py3-none-any.whl\nInstalling collected packages: deeptables\n  Attempting uninstall: deeptables\n    Found existing installation: deeptables 0.2.5\n    Uninstalling deeptables-0.2.5:\n      Successfully uninstalled deeptables-0.2.5\nSuccessfully installed deeptables-0.2.6\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport math\nimport random\nimport warnings\nimport matplotlib.pyplot as plt\nimport numpy as np, pandas as pd, polars as pl\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.preprocessing import MinMaxScaler\nfrom colorama import Fore, Style\n\nimport tensorflow as tf, deeptables as dt\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.optimizers.legacy import Adam\nfrom deeptables.models import DeepTable, ModelConfig\nfrom deeptables.models import deepnets\nfrom catboost import CatBoostRegressor\n\nimport kaggle_evaluation.mcts_inference_server\n\nwarnings.filterwarnings('ignore')\nprint('TensorFlow version:',tf.__version__+',',\n      'GPU =',tf.test.is_gpu_available())\nprint('DeepTables version:',dt.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T16:57:52.359195Z","iopub.execute_input":"2024-11-28T16:57:52.359521Z","iopub.status.idle":"2024-11-28T16:58:05.548093Z","shell.execute_reply.started":"2024-11-28T16:57:52.359491Z","shell.execute_reply":"2024-11-28T16:58:05.547229Z"}},"outputs":[{"name":"stderr","text":"2024-11-28 16:57:53.946126: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-11-28 16:57:53.946185: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-11-28 16:57:53.947894: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"TensorFlow version: 2.15.0, GPU = True\nDeepTables version: 0.2.6\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"seed = 42\ndef seed_everything(seed):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\nseed_everything(seed=seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T16:58:05.549077Z","iopub.execute_input":"2024-11-28T16:58:05.549767Z","iopub.status.idle":"2024-11-28T16:58:05.554859Z","shell.execute_reply.started":"2024-11-28T16:58:05.549709Z","shell.execute_reply":"2024-11-28T16:58:05.554024Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"constant_cols = pd.read_csv('/kaggle/input/um-gps-of-mcts-variants-constant-columns/constant_columns.csv').columns.to_list()\ntarget_col = 'utility_agent1'\ngame_col = 'GameRulesetName'\ngame_rule_cols = ['EnglishRules', 'LudRules']\noutput_cols = ['num_wins_agent1', 'num_draws_agent1', 'num_losses_agent1']\ndropped_cols = ['Id'] + constant_cols + game_rule_cols + output_cols\nagent_cols = ['agent1', 'agent2']\n\ndef preprocess_data(df): \n    df = df.drop(filter(lambda x: x in df.columns, dropped_cols))\n    if CFG.split_agent_features:\n        for col in agent_cols:\n            df = df.with_columns(pl.col(col).str.split(by=\"-\").list.to_struct(fields=lambda idx: f\"{col}_{idx}\")).unnest(col).drop(f\"{col}_0\")\n    df = df.with_columns([pl.col(col).cast(pl.Categorical) for col in df.columns if col[:6] in agent_cols])            \n    df = df.with_columns([pl.col(col).cast(pl.Float32) for col in df.columns if col[:6] not in agent_cols and col != game_col])\n    df = df.to_pandas()\n    print(f'Data shape: {df.shape}\\n')\n    cat_cols = df.select_dtypes(include=['category']).columns.tolist()\n    non_cat_cols = df.select_dtypes(exclude=['category']).columns.tolist()\n    num_cols = [num for num in non_cat_cols if num not in [target_col, game_col]]\n    return df, cat_cols, num_cols","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T16:58:05.556627Z","iopub.execute_input":"2024-11-28T16:58:05.556954Z","iopub.status.idle":"2024-11-28T16:58:05.611406Z","shell.execute_reply.started":"2024-11-28T16:58:05.556929Z","shell.execute_reply":"2024-11-28T16:58:05.610562Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# https://www.kaggle.com/code/cdeotte/tensorflow-transformer-0-790/notebook\nLR_START = 1e-4\nLR_MAX = 1e-3\nLR_MIN = 1e-3\nLR_RAMPUP_EPOCHS = 1\nLR_SUSTAIN_EPOCHS = 0\nEPOCHS = 7\n\ndef lrfn(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        decay_total_epochs = EPOCHS - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS - 1\n        decay_epoch_index = epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS\n        phase = math.pi * decay_epoch_index / decay_total_epochs\n        cosine_decay = 0.5 * (1 + math.cos(phase))\n        lr = (LR_MAX - LR_MIN) * cosine_decay + LR_MIN    \n    return lr\n\nrng = [i for i in range(EPOCHS)]\nlr_y = [lrfn(x) for x in rng]\nplt.figure(figsize=(10, 4))\nplt.plot(rng, lr_y, '-o')\nplt.xlabel('Epoch'); plt.ylabel('LR')\nprint(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\". \\\n      format(lr_y[0], max(lr_y), lr_y[-1]))\nLR_Scheduler = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T16:58:05.612403Z","iopub.execute_input":"2024-11-28T16:58:05.612690Z","iopub.status.idle":"2024-11-28T16:58:05.877928Z","shell.execute_reply.started":"2024-11-28T16:58:05.612650Z","shell.execute_reply":"2024-11-28T16:58:05.877056Z"}},"outputs":[{"name":"stdout","text":"Learning rate schedule: 0.0001 to 0.001 to 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x400 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAA2gAAAFzCAYAAABLmCpMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA/R0lEQVR4nO3dfVjUdaL//9cMCChxExF3ikKthWaiKeKk3couluuJ1gqNgjyevPKkq1/zV+pV2nbcddfddluPJtvNZt5tZnvpMStcV7vZLUBFKTEt3bxBERCJQTBAmPn9gU5LoqIC72F4Pq5rLvIz72FeM281Xr4/8/5YnE6nUwAAAAAA46ymAwAAAAAAGlHQAAAAAMBNUNAAAAAAwE1Q0AAAAADATVDQAAAAAMBNUNAAAAAAwE1Q0AAAAADATVDQAAAAAMBNeJsO4MkcDoeKiooUEBAgi8ViOg4AAAAAQ5xOp06ePKmoqChZredfJ6OgtaGioiJFR0ebjgEAAADATRQWFqpHjx7nvZ+C1oYCAgIkNU5CYGCg4TQAAAAATKmsrFR0dLSrI5wPBa0NnT2tMTAwkIIGAAAA4KIffWKTEAAAAABwExQ0AAAAAHATFDQAAAAAcBMUNAAAAABwExQ0AAAAAHATFDQAAAAAcBNssw90QA0Op7YeKFfpyRqFBfhpSGyIvKwX3rIV7o959TzMqedhTj0T8+p5OvKcGi9oixcv1m9/+1sVFxcrPj5e//u//6shQ4acd/yaNWv03HPP6eDBg+rdu7d+85vf6N5773Xd73Q6NXfuXL366quqqKjQsGHDtGTJEvXu3ds15pe//KXee+895efny8fHRxUVFec8z+HDhzVp0iR9+OGHuuqqq5SRkaH58+fL29v4W4ZOLqvgmH7x7pc6Zq9xHYsM8tPc0X01sl+kwWS4Esyr52FOPQ9z6pmYV8/T0efU6CmOq1ev1vTp0zV37lzt2LFD8fHxSk5OVmlpabPjP/vsM40bN04TJkzQzp07lZKSopSUFBUUFLjGLFiwQAsXLlRmZqZyc3Pl7++v5ORk1dR8P0F1dXV68MEHNWnSpGafp6GhQaNGjVJdXZ0+++wzvfnmm1q6dKnmzJnTum8AcImyCo5p0oodTf7CkaRie40mrdihrIJjhpLhSjCvnoc59TzMqWdiXj2PJ8ypxel0Ok09eWJiohISErRo0SJJksPhUHR0tKZMmaKZM2eeMz41NVXV1dXasGGD69jQoUM1YMAAZWZmyul0KioqSk899ZRmzJghSbLb7QoPD9fSpUs1duzYJt9v6dKlmjZt2jkraB988IF++tOfqqioSOHh4ZKkzMxMPfPMMzp+/Lh8fHxa9PoqKysVFBQku92uwMDAFr8vQHMaHE4N/82Wc/7C+XdXd+uiX6b0k7WDLOFDcjicmr2uQBWnTp93DPPasTCnnoc59UzMq+e52JxaJEUE+emfz9xt5HTHlnYDY+fr1dXVKS8vT7NmzXIds1qtSkpKUnZ2drOPyc7O1vTp05scS05O1rp16yRJBw4cUHFxsZKSklz3BwUFKTExUdnZ2ecUtPPJzs7WzTff7CpnZ59n0qRJ2r17twYOHNjs42pra1VbW+v6dWVlZYueD2iJrQfKL1jOJOnbU6f136t2tlMitBfm1fMwp56HOfVMzKtncUo6Zq/R1gPlsl1/jek452WsoJWVlamhoaFJCZKk8PBw7d27t9nHFBcXNzu+uLjYdf/ZY+cb0xLne55/f47mzJ8/X7/4xS9a/DzApSg9eeFydlZsqL+u8W/ZKi/MO1FdpwNl1Rcdx7x2HMyp52FOPRPz6nlaOqct/ZnKFHa8aEWzZs1qssJXWVmp6Ohog4ngScIC/Fo07lf33+zW/yqEprL/dULjXs256DjmteNgTj0Pc+qZmFfP09I5benPVKYY2yQkNDRUXl5eKikpaXK8pKREERERzT4mIiLiguPPfr2U73kpz/Pvz9EcX19fBQYGNrkBrWVIbIgig87/F4pFjTsUDYkNab9QuGJn5/V8Z8Izrx0Pc+p5mFPPxLx6Hk+ZU2MFzcfHR4MGDdLmzZtdxxwOhzZv3iybzdbsY2w2W5PxkrRp0ybX+NjYWEVERDQZU1lZqdzc3PN+z/M9z65du5rsJrlp0yYFBgaqb9++Lf4+QGvyslo0d3Tzv//O/kU0d3TfDnONDzT693n94cwxrx0Tc+p5mFPPxLx6Hk+ZU6Pb7E+fPl2vvvqq3nzzTe3Zs0eTJk1SdXW1xo8fL0lKT09vsonI1KlTlZWVpRdffFF79+7V888/r+3bt2vy5MmSJIvFomnTpmnevHlav369du3apfT0dEVFRSklJcX1fQ4fPqz8/HwdPnxYDQ0Nys/PV35+vqqqqiRJP/nJT9S3b189+uij+vzzz7Vx40Y9++yzevLJJ+Xr69t+bxDwA3feGKZuPl7nHI8I8tOSR27pENf2wLlG9ovUkkduUcQPVkiZ146LOfU8zKlnYl49jyfMqdFt9iVp0aJFrgtVDxgwQAsXLlRiYqIk6c4771RMTIyWLl3qGr9mzRo9++yzrgtVL1iwoNkLVb/yyiuqqKjQ8OHD9fLLL+uGG25wjXnsscf05ptvnpPlww8/1J133ilJOnTokCZNmqSPPvpI/v7+ysjI0K9//etLulA12+yjta3ZXqj/750vFBXkp98+GK+yqlqFBTQu1bv7vwbh4hocTm09UK7SkzXMq4dgTj0Pc+qZmFfP445z2tJuYLygeTIKGlqT0+nUfyz6VLuO2vXMyDhNuvN605EAAADQQi3tBkZPcQTQcvmFFdp11C4fb6tSE9gdFAAAwBNR0IAOYnn2IUnS6P5RCuF6LAAAAB6JggZ0AGVVtdrwxTFJUrqtl+E0AAAAaCsUNKADWL2tUHUNDsX3CFJ8dLDpOAAAAGgjFDTAzdU3OLQq97AkKd0WYzYMAAAA2hQFDXBzm/eW6mjFdwrx99Go/u5/7Q4AAABcPgoa4ObObg7y0OBo+XU59yLVAAAA8BwUNMCN7S+t0j/3l8lqkdISe5qOAwAAgDZGQQPc2IqcxtWzu+PCFR3SzXAaAAAAtDUKGuCmqmrr9de8I5LYWh8AAKCzoKABbmrtzqM6WVuv60L9NfxHoabjAAAAoB1Q0AA35HQ6tTz7oCTpkaG9ZLVazAYCAABAu6CgAW4o90C5vi6pUtcuXhozqIfpOAAAAGgnFDTADS07s3p2/y3dFdS1i9kwAAAAaDcUNMDNFNtrtHF3iSQ2BwEAAOhsKGiAm1m19bAaHE4NiQ1RXESg6TgAAABoRxQ0wI3U1Tu0KvewJFbPAAAAOiMKGuBGsnYXq6yqVmEBvkq+KcJ0HAAAALQzChrgRs5urf9wYk918eKPJwAAQGfDT4CAm/iyqFLbDn4rb6tFDw/paToOAAAADKCgAW5iec5BSVJyvwiFBfqZDQMAAAAjKGiAG7CfOq21O49KkjJsMWbDAAAAwBgKGuAG1uQVqua0Q3ERAUqIudp0HAAAABhCQQMMczicWpFzSJL0qK2XLBaL4UQAAAAwhYIGGPbJvuM6eOKUAvy8lTKgu+k4AAAAMIiCBhi2PLtx9eyBQT3k7+ttOA0AAABMoqABBhWWn9KWr0olSY8O7WU4DQAAAEyjoAEGrcg5JKdTuq13qK679irTcQAAAGAYBQ0wpOZ0g1ZvL5QkpbO1PgAAAERBA4x59/MiVZw6re7BXXV3XJjpOAAAAHADFDTAAKfTqWVnNgd5ZGgveVnZWh8AAAAUNMCI/MIK7Tpql4+3VakJ0abjAAAAwE1Q0AADzq6eje4fpRB/H8NpAAAA4C4oaEA7K6uq1XtfHJMkpdvYWh8AAADfo6AB7Wz1tkLVNTgU3yNI8dHBpuMAAADAjVDQgHZU3+DQypzG0xvZWh8AAAA/REED2tHmvaUqstcoxN9Ho/pHmo4DAAAAN0NBA9rR8jObgzw0OFp+XbwMpwEAAIC7oaAB7WR/aZX+ub9MVouUltjTdBwAAAC4IQoa0E5WnPns2d1x4YoO6WY4DQAAANwRBQ1oB1W19fpr3hFJUsatbK0PAACA5lHQgHawdudRnayt13Wh/hp2fajpOAAAAHBTFDSgjTmdTi3PPihJemRoL1mtFrOBAAAA4LYoaEAbyz1Qrq9LqtTNx0tjBvUwHQcAAABujIIGtLFlZ1bPUgZ2V1DXLmbDAAAAwK1R0IA2VGyv0cbdJZKkdBubgwAAAODCKGhAG1qVe0gNDqeGxIYoLiLQdBwAAAC4OQoa0Ebq6h1atbVQEqtnAAAAaBkKGtBGsnYXq6yqVmEBvkq+KcJ0HAAAAHQAFDSgjSz77KAk6eHEnurixR81AAAAXBw/NQJt4MuiSm0/9K28rRY9PKSn6TgAAADoIIwXtMWLFysmJkZ+fn5KTEzU1q1bLzh+zZo1iouLk5+fn26++Wa9//77Te53Op2aM2eOIiMj1bVrVyUlJWnfvn1NxpSXlystLU2BgYEKDg7WhAkTVFVV1WTMxo0bNXToUAUEBOjaa6/VmDFjdPDgwVZ5zfB8y3MOSpJG9otQWKCf2TAAAADoMIwWtNWrV2v69OmaO3euduzYofj4eCUnJ6u0tLTZ8Z999pnGjRunCRMmaOfOnUpJSVFKSooKCgpcYxYsWKCFCxcqMzNTubm58vf3V3Jysmpqalxj0tLStHv3bm3atEkbNmzQJ598ookTJ7ruP3DggO677z7dfffdys/P18aNG1VWVqaf/exnbfdmwGPYT53W2p1HJUnpthizYQAAANChWJxOp9PUkycmJiohIUGLFi2SJDkcDkVHR2vKlCmaOXPmOeNTU1NVXV2tDRs2uI4NHTpUAwYMUGZmppxOp6KiovTUU09pxowZkiS73a7w8HAtXbpUY8eO1Z49e9S3b19t27ZNgwcPliRlZWXp3nvv1ZEjRxQVFaV33nlH48aNU21trazWxg777rvv6r777lNtba26dGnZxYYrKysVFBQku92uwEC2WO8sXvvHN5r33h7FRQTog6m3yWKxmI4EAAAAw1raDYytoNXV1SkvL09JSUnfh7FalZSUpOzs7GYfk52d3WS8JCUnJ7vGHzhwQMXFxU3GBAUFKTEx0TUmOztbwcHBrnImSUlJSbJarcrNzZUkDRo0SFarVW+88YYaGhpkt9u1fPlyJSUlXbCc1dbWqrKysskNnYvD4dSKnEOSGlfPKGcAAAC4FMYKWllZmRoaGhQeHt7keHh4uIqLi5t9THFx8QXHn/16sTFhYWFN7vf29lZISIhrTGxsrP72t79p9uzZ8vX1VXBwsI4cOaK33377gq9p/vz5CgoKct2io6MvOB6e55N9x3XwxCkF+HkrZWCU6TgAAADoYIxvEuKOiouL9fjjjysjI0Pbtm3Txx9/LB8fHz3wwAO60Bmhs2bNkt1ud90KCwvbMTXcwfLsxtWzBwb1UDcfb8NpAAAA0NEY+wkyNDRUXl5eKikpaXK8pKREERHNX9Q3IiLiguPPfi0pKVFkZGSTMQMGDHCN+eEmJPX19SovL3c9fvHixQoKCtKCBQtcY1asWKHo6Gjl5uZq6NChzebz9fWVr6/vxV46PFRh+Slt+arx99ajQ3sZTgMAAICOyNgKmo+PjwYNGqTNmze7jjkcDm3evFk2m63Zx9hstibjJWnTpk2u8bGxsYqIiGgyprKyUrm5ua4xNptNFRUVysvLc43ZsmWLHA6HEhMTJUmnTp1ybQ5ylpeXlysj0JwVOYfkdEq39Q7VdddeZToOAAAAOiCjpzhOnz5dr776qt58803t2bNHkyZNUnV1tcaPHy9JSk9P16xZs1zjp06dqqysLL344ovau3evnn/+eW3fvl2TJ0+WJFksFk2bNk3z5s3T+vXrtWvXLqWnpysqKkopKSmSpD59+mjkyJF6/PHHtXXrVn366aeaPHmyxo4dq6ioxs8MjRo1Stu2bdMLL7ygffv2aceOHRo/frx69eqlgQMHtu+bhA6h5nSDVm9vPKWVrfUBAABwuYx+SCY1NVXHjx/XnDlzVFxcrAEDBigrK8u1ycfhw4ebrGTdeuutWrVqlZ599lnNnj1bvXv31rp169SvXz/XmKefflrV1dWaOHGiKioqNHz4cGVlZcnP7/uLBa9cuVKTJ0/WiBEjZLVaNWbMGC1cuNB1/913361Vq1ZpwYIFWrBggbp16yabzaasrCx17dq1Hd4ZdDTrPy9SxanT6h7cVXfHhV38AQAAAEAzjF4HzdNxHbTOwel06j8WfapdR+16ZmScJt15velIAAAAcDNufx00wFPkF1Zo11G7fLytSk3g0goAAAC4fBQ04AotO7O1/uj+UQrx9zGcBgAAAB0ZBQ24AmVVtXrvi2OSpHQbW+sDAADgylDQgCuweluh6hocio8OVnx0sOk4AAAA6OAoaMBlqm9waGVO4+mN6VyYGgAAAK2AggZcps17S1Vkr1GIv49G9Y80HQcAAAAegIIGXKblZzYHSU2Ill8XL8NpAAAA4AkoaMBl2F9apX/uL5PVIqUl9jQdBwAAAB6CggZchhVnPnt2d1y4elzdzXAaAAAAeAoKGnCJqmrr9U7eEUlSxq1sDgIAAIDWQ0EDLtHanUdVVVuv60L9Nez6UNNxAAAA4EEoaMAlcDqdWp59UJL0yNBeslotZgMBAADAo1DQgEuQ8025vi6pUjcfL40Z1MN0HAAAAHgYChpwCZbnHJQkpQzsrqCuXcyGAQAAgMehoAEtVGyv0cbdJZKkdBubgwAAAKD1UdCAFlqVe0gNDqeGxIYoLiLQdBwAAAB4IAoa0AJ19Q6t2looidUzAAAAtB0KGtACWbuLVVZVq7AAXyXfFGE6DgAAADwUBQ1ogWWfHZQkPZzYU128+GMDAACAtsFPmsBF7C6ya/uhb+VttejhIT1NxwEAAIAHo6ABF7E8+5AkaWS/CIUF+hlOAwAAAE9GQQMuwH7qtNblH5UkpdtizIYBAACAx6OgARewJq9QNacdiosIUELM1abjAAAAwMNR0IDzcDicWp7TeHpjui1GFovFcCIAAAB4OgoacB6f7DuuQydOKcDPWykDo0zHAQAAQCdAQQPO4+zmIA8M6qFuPt6G0wAAAKAzoKABzSgsP6UtX5VKkh4d2stwGgAAAHQWFDSgGStyDsnplG7rHarrrr3KdBwAAAB0EhQ04AdqTjdo9fZCSVIGW+sDAACgHVHQgB9Y/3mRKk6dVvfgrrorLsx0HAAAAHQiFDTg3zidTtfmII8M7SUvK1vrAwAAoP1Q0IB/k19YoV1H7fLxtio1Idp0HAAAAHQyFDTg3yw7s3o2un+UQvx9DKcBAABAZ0NBA84oq6rVe18ckySl29haHwAAAO2PggacsXpboeoaHIqPDlZ8dLDpOAAAAOiEKGiApPoGh1bmNJ7emM6FqQEAAGAIBQ2QtHlvqYrsNQrx99Go/pGm4wAAAKCToqABkpZlH5QkpSZEy6+Ll9kwAAAA6LQoaOj09pdW6dP9J2S1SGmJPU3HAQAAQCdGQUOnt+LMZ8/ujgtXj6u7GU4DAACAzoyChk6tqrZe7+QdkSRl3MrmIAAAADCLgoZObe3Oo6qqrdd1of4adn2o6TgAAADo5Cho6LScTqeWn9kc5FFbL1mtFrOBAAAA0OlR0NBp5XxTrq9LqtTNx0tjBvUwHQcAAACgoKHzWp5zUJKUMrC7Av26mA0DAAAAiIKGTuqY/Ttt3F0iSUq3sTkIAAAA3AMFDZ3SX3IPq8Hh1JDYEMVFBJqOAwAAAEiioKETqqt3aNXWQkmsngEAAMC9UNDQ6XxQcExlVbUKC/BV8k0RpuMAAAAALhQ0dDrLsw9Jkh5O7KkuXvwRAAAAgPvgp1N0KruL7Np+6Ft5Wy16eEhP03EAAACAJowXtMWLFysmJkZ+fn5KTEzU1q1bLzh+zZo1iouLk5+fn26++Wa9//77Te53Op2aM2eOIiMj1bVrVyUlJWnfvn1NxpSXlystLU2BgYEKDg7WhAkTVFVVdc73+d3vfqcbbrhBvr6+6t69u375y1+2zouGMWdXz0b2i1BYoJ/hNAAAAEBTRgva6tWrNX36dM2dO1c7duxQfHy8kpOTVVpa2uz4zz77TOPGjdOECRO0c+dOpaSkKCUlRQUFBa4xCxYs0MKFC5WZmanc3Fz5+/srOTlZNTU1rjFpaWnavXu3Nm3apA0bNuiTTz7RxIkTmzzX1KlT9dprr+l3v/ud9u7dq/Xr12vIkCFt80agXdhPnda6/KOSpHRbjNkwAAAAQDMsTqfTaerJExMTlZCQoEWLFkmSHA6HoqOjNWXKFM2cOfOc8ampqaqurtaGDRtcx4YOHaoBAwYoMzNTTqdTUVFReuqppzRjxgxJkt1uV3h4uJYuXaqxY8dqz5496tu3r7Zt26bBgwdLkrKysnTvvffqyJEjioqK0p49e9S/f38VFBToxhtvvOzXV1lZqaCgINntdgUGspW7aa/94xvNe2+P4iIC9MHU22SxWExHAgAAQCfR0m5gbAWtrq5OeXl5SkpK+j6M1aqkpCRlZ2c3+5js7Owm4yUpOTnZNf7AgQMqLi5uMiYoKEiJiYmuMdnZ2QoODnaVM0lKSkqS1WpVbm6uJOndd9/Vddddpw0bNig2NlYxMTH6r//6L5WXl1/wNdXW1qqysrLJDe7B4XBqeU7j6Y3pthjKGQAAANySsYJWVlamhoYGhYeHNzkeHh6u4uLiZh9TXFx8wfFnv15sTFhYWJP7vb29FRIS4hrzzTff6NChQ1qzZo2WLVumpUuXKi8vTw888MAFX9P8+fMVFBTkukVHR19wPNrPJ/uO69CJUwrw81bKwCjTcQAAAIBmGd8kxB05HA7V1tZq2bJluu2223TnnXfq9ddf14cffqivvvrqvI+bNWuW7Ha761ZYWNiOqXEhZzcHeXBQtLr5eBtOAwAAADTPWEELDQ2Vl5eXSkpKmhwvKSlRRETzFw+OiIi44PizXy825oebkNTX16u8vNw1JjIyUt7e3rrhhhtcY/r06SNJOnz48Hlfk6+vrwIDA5vcYF5h+Slt+apxzh+19TKcBgAAADg/YwXNx8dHgwYN0ubNm13HHA6HNm/eLJvN1uxjbDZbk/GStGnTJtf42NhYRURENBlTWVmp3Nxc1xibzaaKigrl5eW5xmzZskUOh0OJiYmSpGHDhqm+vl7/+te/XGO+/vprSVKvXvyA39GsyDkkp1O6rXeoYkP9TccBAAAAzsvouV7Tp09XRkaGBg8erCFDhuill15SdXW1xo8fL0lKT09X9+7dNX/+fEmNW9/fcccdevHFFzVq1Ci99dZb2r59u1555RVJksVi0bRp0zRv3jz17t1bsbGxeu655xQVFaWUlBRJjSthI0eO1OOPP67MzEydPn1akydP1tixYxUV1fjZpKSkJN1yyy36z//8T7300ktyOBx68skn9eMf/7jJqhrcX83pBq3e3niqaQZb6wMAAMDNGS1oqampOn78uObMmaPi4mINGDBAWVlZrk0+Dh8+LKv1+0W+W2+9VatWrdKzzz6r2bNnq3fv3lq3bp369evnGvP000+rurpaEydOVEVFhYYPH66srCz5+X1/UeKVK1dq8uTJGjFihKxWq8aMGaOFCxe67rdarXr33Xc1ZcoU3X777fL399c999yjF198sR3eFbSm9Z8XqeLUaXUP7qq74sIu/gAAAADAIKPXQfN0XAfNLKfTqdGL/qmCo5V6ZmScJt15velIAAAA6KTc/jpoQFvbWVihgqOV8vG2KjWBSx4AAADA/VHQ4LHObq0/un+UQvx9DKcBAAAALo6CBo9UVlWr9744JklKZ2t9AAAAdBAUNHik1dsKVdfgUHx0sOKjg03HAQAAAFqkVQvajh079NOf/rQ1vyVwyeobHFqZ03h6Y/pQVs8AAADQcVxyQdu4caNmzJih2bNn65tvvpEk7d27VykpKUpISJDD4Wj1kMCl2Ly3VEX2GoX4+2hU/0jTcQAAAIAWu6TroL3++ut6/PHHFRISom+//Vavvfaafv/732vKlClKTU1VQUGB+vTp01ZZgRZZln1QkpSaEC2/Ll5mwwAAAACX4JJW0P74xz/qN7/5jcrKyvT222+rrKxML7/8snbt2qXMzEzKGYzbX3pSn+4/IatFSkvsaToOAAAAcEkuqaD961//0oMPPihJ+tnPfiZvb2/99re/VY8ePdokHHCpzm6tP6JPuHpc3c1wGgAAAODSXFJB++6779StW+MPvRaLRb6+voqM5DM+cA9VtfX6646jkthaHwAAAB3TJX0GTZJee+01XXXVVZKk+vp6LV26VKGhoU3G/PznP2+ddMAlWLvzqKpq63VdqL+GXR968QcAAAAAbsbidDqdLR0cExMji8Vy4W9osbh2d+zsKisrFRQUJLvdrsDAQNNxPJrT6dRP/vCJ9pVWae7ovho/LNZ0JAAAAMClpd3gklbQDh48eMH7jxw5ohdeeOFSviXQKnK+Kde+0ip18/HSmEF8JhIAAAAdU6teqPrEiRN6/fXXW/NbAi2yPOegJCllYHcF+nUxGwYAAAC4TK1a0AATjtm/08bdJZLYHAQAAAAdGwUNHd5fcg+rweHUkNgQxUXwWT8AAAB0XBQ0dGh19Q6t2looidUzAAAAdHyXtEnIz372swveX1FRcSVZgEv2QcExlVXVKizAV8k3RZiOAwAAAFyRSypoQUFBF70/PT39igIBl2J59iFJ0sOJPdXFiwVhAAAAdGyXVNDeeOONtsoBXLLdRXZtP/StvK0WPTykp+k4AAAAwBVjyQEd1tnVs5H9IhQW6Gc4DQAAAHDlKGjokOynTmtd/lFJUrotxmwYAAAAoJVQ0NAhrckrVM1ph+IiApQQc7XpOAAAAECroKChw3E4nFqe03h6Y7otRhaLxXAiAAAAoHVQ0NDhfLLvuA6dOKUAP2+lDIwyHQcAAABoNRQ0dDjLzmwO8uCgaHXzuaSNSAEAAAC3RkFDh1JYfkofflUqSXrU1stwGgAAAKB1UdDQoazIOSSnU7qtd6hiQ/1NxwEAAABaFQUNHUbN6Qat3l4oScpga30AAAB4IAoaOoz1nxep4tRpdQ/uqrviwkzHAQAAAFodBQ0dgtPp1LLsg5IaP3vmZWVrfQAAAHgeCho6hJ2FFSo4Wikfb6seGhxtOg4AAADQJiho6BCWn9laf3T/KIX4+xhOAwAAALQNChrcXllVrd774pgkKeNWttYHAACA56Kgwe2t3laougaH4qOD1b9HsOk4AAAAQJuhoMGt1Tc4tDKn8fTG9KGsngEAAMCzUdDg1v6+p1RF9hqF+PtoVP9I03EAAACANkVBg1tbnnNQkpSaEC2/Ll5mwwAAAABtjIIGt7W/9KQ+3X9CVouUltjTdBwAAACgzVHQ4LbObq0/ok+4elzdzXAaAAAAoO1R0OCWqmrr9dcdRyVJ6TY2BwEAAEDnQEGDW1q786iqaut1Xai/hl0fajoOAAAA0C4oaHA7TqdTyz47KEl61NZLVqvFbCAAAACgnVDQ4HZyvinXvtIqdfPx0phBPUzHAQAAANoNBQ1u5+zW+vcP7K5Avy5mwwAAAADtiIIGt3LM/p027i6RJKXbYsyGAQAAANoZBQ1u5S+5h9XgcGpIbIhujAgwHQcAAABoVxQ0uI26eodWbS2UJGWwegYAAIBOiIIGt/FBwTGVVdUqPNBXP7kp3HQcAAAAoN1R0OA2lmcfkiSNG9JTXbz4rQkAAIDOh5+C4RZ2F9m1/dC38rZa9PCQnqbjAAAAAEa4RUFbvHixYmJi5Ofnp8TERG3duvWC49esWaO4uDj5+fnp5ptv1vvvv9/kfqfTqTlz5igyMlJdu3ZVUlKS9u3b12RMeXm50tLSFBgYqODgYE2YMEFVVVXNPt/+/fsVEBCg4ODgK3qdOL+zq2cj+0UoLNDPcBoAAADADOMFbfXq1Zo+fbrmzp2rHTt2KD4+XsnJySotLW12/GeffaZx48ZpwoQJ2rlzp1JSUpSSkqKCggLXmAULFmjhwoXKzMxUbm6u/P39lZycrJqaGteYtLQ07d69W5s2bdKGDRv0ySefaOLEiec83+nTpzVu3Djddtttrf/iIUmynzqtdflHJbG1PgAAADo3i9PpdJoMkJiYqISEBC1atEiS5HA4FB0drSlTpmjmzJnnjE9NTVV1dbU2bNjgOjZ06FANGDBAmZmZcjqdioqK0lNPPaUZM2ZIkux2u8LDw7V06VKNHTtWe/bsUd++fbVt2zYNHjxYkpSVlaV7771XR44cUVRUlOt7P/PMMyoqKtKIESM0bdo0VVRUtPi1VVZWKigoSHa7XYGBgZfz9nQKr/3jG817b4/iIgL0wdTbZLFYTEcCAAAAWlVLu4HRFbS6ujrl5eUpKSnJdcxqtSopKUnZ2dnNPiY7O7vJeElKTk52jT9w4ICKi4ubjAkKClJiYqJrTHZ2toKDg13lTJKSkpJktVqVm5vrOrZlyxatWbNGixcvbtHrqa2tVWVlZZMbLszhcGp5TuPpjem2GMoZAAAAOjWjBa2srEwNDQ0KD2+6pXp4eLiKi4ubfUxxcfEFx5/9erExYWFhTe739vZWSEiIa8yJEyf02GOPaenSpS1e/Zo/f76CgoJct+jo6BY9rjP7ZN9xHTpxSgF+3koZGHXxBwAAAAAezPhn0NzV448/rocffli33357ix8za9Ys2e12162wsLANE3qGZWc2B3lwULS6+XgbTgMAAACYZbSghYaGysvLSyUlJU2Ol5SUKCIiotnHREREXHD82a8XG/PDTUjq6+tVXl7uGrNlyxb97ne/k7e3t7y9vTVhwgTZ7XZ5e3vrz3/+c7PZfH19FRgY2OSG8zt84pQ+/KpxHh619TKcBgAAADDPaEHz8fHRoEGDtHnzZtcxh8OhzZs3y2azNfsYm83WZLwkbdq0yTU+NjZWERERTcZUVlYqNzfXNcZms6miokJ5eXmuMVu2bJHD4VBiYqKkxs+p5efnu24vvPCCAgIClJ+fr/vvv7913oBObkXuITmd0u03XKvYUH/TcQAAAADjjJ9TNn36dGVkZGjw4MEaMmSIXnrpJVVXV2v8+PGSpPT0dHXv3l3z58+XJE2dOlV33HGHXnzxRY0aNUpvvfWWtm/frldeeUWSZLFYNG3aNM2bN0+9e/dWbGysnnvuOUVFRSklJUWS1KdPH40cOVKPP/64MjMzdfr0aU2ePFljx4517eDYp0+fJjm3b98uq9Wqfv36tdM749lqTjfo7e2Np4CmD2X1DAAAAJDcoKClpqbq+PHjmjNnjoqLizVgwABlZWW5Nvk4fPiwrNbvF/puvfVWrVq1Ss8++6xmz56t3r17a926dU2K09NPP63q6mpNnDhRFRUVGj58uLKysuTn9/0FkFeuXKnJkydrxIgRslqtGjNmjBYuXNh+L7yTW/95kSpOnVb34K66Ky7s4g8AAAAAOgHj10HzZFwHrXlOp1OjF/1TBUcrNfOeOD1xx/WmIwEAAABtqkNcBw2d087CChUcrZSPt1UPDeZSBAAAAMBZFDS0u+VnttYf3T9KIf4+htMAAAAA7oOChnZVVlWr9744JknKuJXNQQAAAIB/R0FDu1q9rVB1DQ7FRwerf49g03EAAAAAt0JBQ7upb3BoZU7j6Y1srQ8AAACci4KGdvP3PaUqstcoxN9Ho/pHmo4DAAAAuB0KGtrN8pyDkqTUhGj5dfEyGwYAAABwQxQ0tIv9pSf16f4TslqktMSepuMAAAAAbomChnZxdmv9EX3C1ePqbobTAAAAAO6JgoY2V1Vbr7/uOCpJSrexOQgAAABwPhQ0tLm1O46oqrZe113rr2HXh5qOAwAAALgtChralNPp1LIzpzc+OrSXrFaL4UQAAACA+6KgoU3lfFOufaVV6ubjpTGDepiOAwAAALg1Chra1LLsg5Kk+wd2V6BfF7NhAAAAADdHQUObOWb/Tn/7skSSlG6LMRsGAAAA6AAoaGgzf8k9rAaHU0NiQ3RjRIDpOAAAAIDbo6ChTdTVO7Rqa6EkKYPVMwAAAKBFKGhoEx8UHFNZVa3CA331k5vCTccBAAAAOgQKGtrE8jNb648b0lNdvPhtBgAAALQEPzmj1e0usmv7oW/lbbXo4SE9TccBAAAAOgwKGlrd2dWzkf0iFBboZzgNAAAA0HFQ0NCq7KdOa13+UUlSxq0xZsMAAAAAHQwFDa1qTV6hak47FBcRoMG9rjYdBwAAAOhQKGhoNQ6HU8tzGk9vTLfFyGKxGE4EAAAAdCwUNLSaj/cd16ETpxTg562UgVGm4wAAAAAdDgUNrebs5iAPDopWNx9vw2kAAACAjoeChlZx+MQpffhVqSTpUVsvw2kAAACAjomChlaxIveQnE7p9huuVWyov+k4AAAAQIdEQcMVqzndoLe3F0qS0oeyegYAAABcLgoartj6z4tUceq0ugd31V1xYabjAAAAAB0WBQ1XxOl0aln2QUmNnz3zsrK1PgAAAHC5KGi4IjsLK1RwtFI+3lY9NDjadBwAAACgQ6Og4Yqc3Vr/P+KjFOLvYzgNAAAA0LFR0HDZyqpq9d4XxyRJ6WytDwAAAFwxChou2+pthaprcCg+Olj9ewSbjgMAAAB0eBQ0XJb6BodW5DSe3pjB6hkAAADQKihouCx/31OqY/Yahfj76N6bI03HAQAAADwCBQ2XZXnOQUlSakK0/Lp4mQ0DAAAAeAgKGi7Z/tKT+nT/CVktUlpiT9NxAAAAAI9BQcMlO7u1/og+4epxdTfDaQAAAADPQUHDJamqrddfdxyVxNb6AAAAQGujoOGSrN1xRFW19bruWn8Nuz7UdBwAAADAo1DQ0GJOp1PLzpze+OjQXrJaLYYTAQAAAJ6FgoYWy/mmXPtKq9TNx0tjBvUwHQcAAADwOBQ0tNiy7IOSpPsHdlegXxezYQAAAAAPREFDixyzf6e/fVkiSUq3xZgNAwAAAHgoChpaZFXuYTU4nEqMDdGNEQGm4wAAAAAeiYKGi6qrd+gvWwslsXoGAAAAtCUKGi7qg4JjKquqVXigr35yU7jpOAAAAIDHoqDhos5urf/wkF7q4sVvGQAAAKCt8NM2Lmh3kV15h76Vt9WicUOiTccBAAAAPJpbFLTFixcrJiZGfn5+SkxM1NatWy84fs2aNYqLi5Ofn59uvvlmvf/++03udzqdmjNnjiIjI9W1a1clJSVp3759TcaUl5crLS1NgYGBCg4O1oQJE1RVVeW6/6OPPtJ9992nyMhI+fv7a8CAAVq5cmXrvegOYvmZ1bOR/SIUFuhnOA0AAADg2YwXtNWrV2v69OmaO3euduzYofj4eCUnJ6u0tLTZ8Z999pnGjRunCRMmaOfOnUpJSVFKSooKCgpcYxYsWKCFCxcqMzNTubm58vf3V3Jysmpqalxj0tLStHv3bm3atEkbNmzQJ598ookTJzZ5nv79++uvf/2rvvjiC40fP17p6enasGFD270ZbsZ+6rTW5R+VJGXcGmM2DAAAANAJWJxOp9NkgMTERCUkJGjRokWSJIfDoejoaE2ZMkUzZ848Z3xqaqqqq6ubFKWhQ4dqwIAByszMlNPpVFRUlJ566inNmDFDkmS32xUeHq6lS5dq7Nix2rNnj/r27att27Zp8ODBkqSsrCzde++9OnLkiKKioprNOmrUKIWHh+vPf/5zi15bZWWlgoKCZLfbFRgYeEnvizt47R/faN57exQXEaAPpt4mi8ViOhIAAADQIbW0GxhdQaurq1NeXp6SkpJcx6xWq5KSkpSdnd3sY7Kzs5uMl6Tk5GTX+AMHDqi4uLjJmKCgICUmJrrGZGdnKzg42FXOJCkpKUlWq1W5ubnnzWu32xUSEnLpL7QDcjicWp7TeHpjui2GcgYAAAC0A2+TT15WVqaGhgaFhzfduj08PFx79+5t9jHFxcXNji8uLnbdf/bYhcaEhYU1ud/b21shISGuMT/09ttva9u2bfrTn/503tdTW1ur2tpa168rKyvPO9bdfbzvuA6dOKUAP2+lDGx+RREAAABA6zL+GbSO4MMPP9T48eP16quv6qabbjrvuPnz5ysoKMh1i47uuLsent0c5MFB0ermY7THAwAAAJ2G0YIWGhoqLy8vlZSUNDleUlKiiIiIZh8TERFxwfFnv15szA83Iamvr1d5efk5z/vxxx9r9OjR+sMf/qD09PQLvp5Zs2bJbre7boWFhRcc764OnzilD79qfH8etfUynAYAAADoPIwWNB8fHw0aNEibN292HXM4HNq8ebNsNluzj7HZbE3GS9KmTZtc42NjYxUREdFkTGVlpXJzc11jbDabKioqlJeX5xqzZcsWORwOJSYmuo599NFHGjVqlH7zm9802eHxfHx9fRUYGNjk1hGtyD0kp1O6/YZrFRvqbzoOAAAA0GkYP3dt+vTpysjI0ODBgzVkyBC99NJLqq6u1vjx4yVJ6enp6t69u+bPny9Jmjp1qu644w69+OKLGjVqlN566y1t375dr7zyiiTJYrFo2rRpmjdvnnr37q3Y2Fg999xzioqKUkpKiiSpT58+GjlypB5//HFlZmbq9OnTmjx5ssaOHevawfHDDz/UT3/6U02dOlVjxoxxfTbNx8fHozcK+a6uQau3Na78pQ9l9QwAAABoT8YLWmpqqo4fP645c+aouLhYAwYMUFZWlmuTj8OHD8tq/X6h79Zbb9WqVav07LPPavbs2erdu7fWrVunfv36ucY8/fTTqq6u1sSJE1VRUaHhw4crKytLfn7fX2h55cqVmjx5skaMGCGr1aoxY8Zo4cKFrvvffPNNnTp1SvPnz3eVQ0m644479NFHH7XhO2LWu58Xyf7dafW4uqvuigu7+AMAAAAAtBrj10HzZB3tOmhOp1OjF/1TBUcrNfOeOD1xx/WmIwEAAAAeoUNcBw3uZWdhhQqOVsrH26qHBnfcHSgBAACAjoqCBpdlnx2UJP1HfJRC/H3MhgEAAAA6IQoaJEllVbV6f1fjRijpbK0PAAAAGEFBgyRp9bZC1TU4FB8drP49gk3HAQAAADolChpU3+DQipxDkqQMVs8AAAAAYyho0N/3lOqYvUYh/j669+ZI03EAAACATouCBi3POShJSk2Ill8XL7NhAAAAgE6MgtbJ7S89qU/3n5DVIqUl9jQdBwAAAOjUKGid3PLsxs+ejegTrh5XdzOcBgAAAOjcKGidWFVtvf6646gkKcMWYzYMAAAAAApaZ7Z2xxFV1dbrumv9NexH15iOAwAAAHR6FLROyul0atmZ0xsfHdpLFovFcCIAAAAAFLROKvubE9pXWqVuPl4aM6iH6TgAAAAAREHrtM5uDnL/wO4K9OtiOA0AAAAAiYLWKR2zf6e/fVkiSUpncxAAAADAbVDQOqFVuYfV4HAqMTZEN0YEmI4DAAAA4AwKWidTV+/QX7YWSmL1DAAAAHA3FLRO5oOCYyqrqlV4oK9+clO46TgAAAAA/g0FrZM5u7X+w0N6qYsX0w8AAAC4E35C70QKjtqVd+hbeVstGjck2nQcAAAAAD/gbToA2l6Dw6mtB8r1x79/LUlKvilcYYF+hlMBAAAA+CEKmofLKjimX7z7pY7Za1zHcr4pV1bBMY3sF2kwGQAAAIAf4hRHD5ZVcEyTVuxoUs4kqby6TpNW7FBWwTFDyQAAAAA0h4LmoRocTv3i3S/lbOa+s8d+8e6XanA0NwIAAACACRQ0D7X1QPk5K2f/zinpmL1GWw+Ut18oAAAAABdEQfNQpSfPX84uZxwAAACAtkdB81BhAS3bpbGl4wAAAAC0PQqahxoSG6LIID9ZznO/RVJkkJ+GxIa0ZywAAAAAF0BB81BeVovmju4rSeeUtLO/nju6r7ys56twAAAAANobBc2DjewXqSWP3KKIoKanMUYE+WnJI7dwHTQAAADAzXChag83sl+kftw3QlsPlKv0ZI3CAhpPa2TlDAAAAHA/FLROwMtqke36a0zHAAAAAHARnOIIAAAAAG6CggYAAAAAboKCBgAAAABugoIGAAAAAG6CggYAAAAAboKCBgAAAABugm3225DT6ZQkVVZWGk4CAAAAwKSzneBsRzgfClobOnnypCQpOjracBIAAAAA7uDkyZMKCgo67/0W58UqHC6bw+FQUVGRAgICZLFYjGaprKxUdHS0CgsLFRgYaDQLWgdz6pmYV8/DnHoe5tQzMa+ex93m1Ol06uTJk4qKipLVev5PmrGC1oasVqt69OhhOkYTgYGBbvEbFK2HOfVMzKvnYU49D3PqmZhXz+NOc3qhlbOz2CQEAAAAANwEBQ0AAAAA3AQFrZPw9fXV3Llz5evrazoKWglz6pmYV8/DnHoe5tQzMa+ep6POKZuEAAAAAICbYAUNAAAAANwEBQ0AAAAA3AQFDQAAAADcBAUNAAAAANwEBa0TWLx4sWJiYuTn56fExERt3brVdCRcgU8++USjR49WVFSULBaL1q1bZzoSrtD8+fOVkJCggIAAhYWFKSUlRV999ZXpWLhCS5YsUf/+/V0XSLXZbPrggw9Mx0Ir+vWvfy2LxaJp06aZjoIr8Pzzz8tisTS5xcXFmY6FK3T06FE98sgjuuaaa9S1a1fdfPPN2r59u+lYLUJB83CrV6/W9OnTNXfuXO3YsUPx8fFKTk5WaWmp6Wi4TNXV1YqPj9fixYtNR0Er+fjjj/Xkk08qJydHmzZt0unTp/WTn/xE1dXVpqPhCvTo0UO//vWvlZeXp+3bt+vuu+/Wfffdp927d5uOhlawbds2/elPf1L//v1NR0EruOmmm3Ts2DHX7Z///KfpSLgC3377rYYNG6YuXbrogw8+0JdffqkXX3xRV199teloLcI2+x4uMTFRCQkJWrRokSTJ4XAoOjpaU6ZM0cyZMw2nw5WyWCxau3atUlJSTEdBKzp+/LjCwsL08ccf6/bbbzcdB60oJCREv/3tbzVhwgTTUXAFqqqqdMstt+jll1/WvHnzNGDAAL300kumY+EyPf/881q3bp3y8/NNR0ErmTlzpj799FP94x//MB3lsrCC5sHq6uqUl5enpKQk1zGr1aqkpCRlZ2cbTAbgQux2u6TGH+bhGRoaGvTWW2+purpaNpvNdBxcoSeffFKjRo1q8v9XdGz79u1TVFSUrrvuOqWlpenw4cOmI+EKrF+/XoMHD9aDDz6osLAwDRw4UK+++qrpWC1GQfNgZWVlamhoUHh4eJPj4eHhKi4uNpQKwIU4HA5NmzZNw4YNU79+/UzHwRXatWuXrrrqKvn6+uqJJ57Q2rVr1bdvX9OxcAXeeust7dixQ/PnzzcdBa0kMTFRS5cuVVZWlpYsWaIDBw7otttu08mTJ01Hw2X65ptvtGTJEvXu3VsbN27UpEmT9POf/1xvvvmm6Wgt4m06AADge08++aQKCgr4/IOHuPHGG5Wfny+73a533nlHGRkZ+vjjjylpHVRhYaGmTp2qTZs2yc/Pz3QctJJ77rnH9d/9+/dXYmKievXqpbfffpvTkTsoh8OhwYMH61e/+pUkaeDAgSooKFBmZqYyMjIMp7s4VtA8WGhoqLy8vFRSUtLkeElJiSIiIgylAnA+kydP1oYNG/Thhx+qR48epuOgFfj4+OhHP/qRBg0apPnz5ys+Pl5//OMfTcfCZcrLy1NpaaluueUWeXt7y9vbWx9//LEWLlwob29vNTQ0mI6IVhAcHKwbbrhB+/fvNx0FlykyMvKcfwjr06dPhzl1lYLmwXx8fDRo0CBt3rzZdczhcGjz5s18BgJwI06nU5MnT9batWu1ZcsWxcbGmo6ENuJwOFRbW2s6Bi7TiBEjtGvXLuXn57tugwcPVlpamvLz8+Xl5WU6IlpBVVWV/vWvfykyMtJ0FFymYcOGnXO5mq+//lq9evUylOjScIqjh5s+fboyMjI0ePBgDRkyRC+99JKqq6s1fvx409Fwmaqqqpr8q96BAweUn5+vkJAQ9ezZ02AyXK4nn3xSq1at0v/93/8pICDA9RnRoKAgde3a1XA6XK5Zs2bpnnvuUc+ePXXy5EmtWrVKH330kTZu3Gg6Gi5TQEDAOZ8N9ff31zXXXMNnRjuwGTNmaPTo0erVq5eKioo0d+5ceXl5ady4caaj4TL9v//3/3TrrbfqV7/6lR566CFt3bpVr7zyil555RXT0VqEgubhUlNTdfz4cc2ZM0fFxcUaMGCAsrKyztk4BB3H9u3bddddd7l+PX36dElSRkaGli5daigVrsSSJUskSXfeeWeT42+88YYee+yx9g+EVlFaWqr09HQdO3ZMQUFB6t+/vzZu3Kgf//jHpqMB+DdHjhzRuHHjdOLECV177bUaPny4cnJydO2115qOhsuUkJCgtWvXatasWXrhhRcUGxurl156SWlpaaajtQjXQQMAAAAAN8Fn0AAAAADATVDQAAAAAMBNUNAAAAAAwE1Q0AAAAADATVDQAAAAAMBNUNAAAAAAwE1Q0AAAAADATVDQAABwQxaLRevWrTMdAwDQzihoAAD8wGOPPSaLxXLObeTIkaajAQA8nLfpAAAAuKORI0fqjTfeaHLM19fXUBoAQGfBChoAAM3w9fVVREREk9vVV18tqfH0wyVLluiee+5R165ddd111+mdd95p8vhdu3bp7rvvVteuXXXNNddo4sSJqqqqajLmz3/+s2666Sb5+voqMjJSkydPbnJ/WVmZ7r//fnXr1k29e/fW+vXr2/ZFAwCMo6ABAHAZnnvuOY0ZM0aff/650tLSNHbsWO3Zs0eSVF1dreTkZF199dXatm2b1qxZo7///e9NCtiSJUv05JNPauLEidq1a5fWr1+vH/3oR02e4xe/+IUeeughffHFF7r33nuVlpam8vLydn2dAID2ZXE6nU7TIQAAcCePPfaYVqxYIT8/vybHZ8+erdmzZ8tiseiJJ57QkiVLXPcNHTpUt9xyi15++WW9+uqreuaZZ1RYWCh/f39J0vvvv6/Ro0erqKhI4eHh6t69u8aPH6958+Y1m8FisejZZ5/V//zP/0hqLH1XXXWVPvjgAz4LBwAejM+gAQDQjLvuuqtJAZOkkJAQ13/bbLYm99lsNuXn50uS9uzZo/j4eFc5k6Rhw4bJ4XDoq6++ksViUVFRkUaMGHHBDP3793f9t7+/vwIDA1VaWnq5LwkA0AFQ0AAAaIa/v/85pxy2lq5du7ZoXJcuXZr82mKxyOFwtEUkAICb4DNoAABchpycnHN+3adPH0lSnz599Pnnn6u6utp1/6effiqr1aobb7xRAQEBiomJ0ebNm9s1MwDA/bGCBgBAM2pra1VcXNzkmLe3t0JDQyVJa9as0eDBgzV8+HCtXLlSW7du1euvvy5JSktL09y5c5WRkaHnn39ex48f15QpU/Too48qPDxckvT888/riSeeUFhYmO655x6dPHlSn376qaZMmdK+LxQA4FYoaAAANCMrK0uRkZFNjt14443au3evpMYdFt966y3993//tyIjI/WXv/xFffv2lSR169ZNGzdu1NSpU5WQkKBu3bppzJgx+v3vf+/6XhkZGaqpqdEf/vAHzZgxQ6GhoXrggQfa7wUCANwSuzgCAHCJLBaL1q5dq5SUFNNRAAAehs+gAQAAAICboKABAAAAgJvgM2gAAFwiPh0AAGgrrKABAAAAgJugoAEAAACAm6CgAQAAAICboKABAAAAgJugoAEAAACAm6CgAQAAAICboKABAAAAgJugoAEAAACAm6CgAQAAAICb+P8BhNY7CvsuMJYAAAAASUVORK5CYII="},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"class CFG:\n    train_path = '/kaggle/input/um-game-playing-strength-of-mcts-variants/train.csv'\n    split_agent_features = True\n    scaler = MinMaxScaler()  # Scaler or None\n    \n    nn = True\n    ctb = True\n    ens_weights = {'nn': 0.40, 'ctb': 0.60}  # While nn = True and ctb = True\n    \n    folds = 6\n    \n    epochs = 7\n    batch_size = 128\n    LR_Scheduler = [LR_Scheduler]\n    optimizer = Adam(learning_rate=1e-3)\n    conf = ModelConfig(auto_imputation=False,\n                       auto_discrete=False,\n                       auto_discard_unique=True,\n                       categorical_columns='auto',\n                       apply_gbm_features=True,\n                       fixed_embedding_dim=True,\n                       embeddings_output_dim=4,\n                       embedding_dropout=0.2,\n                       nets=['dnn_nets'] + ['fm_nets'] + ['cin_nets'],\n                       dnn_params={\n                           'hidden_units': ((1024, 0.0, True),\n                                            (512, 0.0, True),\n                                            (256, 0.0, True),\n                                            (128, 0.0, True)),\n                           'dnn_activation': 'relu',\n                       },\n                       stacking_op='concat',\n                       output_use_bias=False,\n                       optimizer=optimizer,\n                       task='regression',\n                       loss='auto',\n                       metrics=[\"RootMeanSquaredError\"],\n                       earlystopping_patience=1,\n                       )\n\n    ctb_params = dict(iterations=2000,\n                      learning_rate=0.08,\n                      depth=10,\n                      l2_leaf_reg=.054,\n                      random_strength=0.3,\n                      bagging_temperature=0.5,\n                      loss_function='RMSE',\n                      eval_metric = 'RMSE',\n                      metric_period=500,\n                      od_type='Iter',\n                      od_wait=100,\n                      task_type='GPU',\n                      allow_writing_files=False,\n                      )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T16:58:05.879203Z","iopub.execute_input":"2024-11-28T16:58:05.879582Z","iopub.status.idle":"2024-11-28T16:58:05.888011Z","shell.execute_reply.started":"2024-11-28T16:58:05.879539Z","shell.execute_reply":"2024-11-28T16:58:05.887218Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def train(data, cat_cols, num_cols, scaler):\n    cv = GroupKFold(n_splits=CFG.folds)\n    groups = data[game_col]\n    X = data.drop([target_col, game_col], axis=1)\n    y = data[target_col]\n    oof = np.zeros(len(data))\n    nn_models = []\n    ctb_models = []\n    \n    print('nn = '+str(CFG.nn))\n    print('ctb = '+str(CFG.ctb),'\\n')\n    \n    for fi, (train_idx, valid_idx) in enumerate(cv.split(X, y, groups)):\n        print(\"#\"*25)\n        print(f\"### Fold {fi+1}/{CFG.folds} ...\")\n        print(\"#\"*25)\n        \n        os.makedirs(f\"/kaggle/working/nn_models/fold{fi}\", exist_ok=True)\n        os.makedirs(f\"/tmp/workdir/kaggle/working/nn_models/fold{fi}\", exist_ok=True)\n        os.makedirs(f\"/kaggle/working/ctb_models/fold{fi}\", exist_ok=True)\n\n        if CFG.nn == True and CFG.ctb == False:\n            print('\\n',\"nn only model training.\",'\\n')\n            K.clear_session()\n            nn_model = DeepTable(config=CFG.conf)\n            nn_model.fit(X.iloc[train_idx], y.iloc[train_idx],\n                      validation_data=(X.iloc[valid_idx], y.iloc[valid_idx]),\n                      callbacks=CFG.LR_Scheduler,\n                      batch_size=CFG.batch_size, epochs=CFG.epochs, verbose=2)\n            nn_models.append(nn_model)\n            \n            # Save model\n            nn_model.save(f'/kaggle/working/nn_models/fold{fi}')\n            os.system(f'cp -r /kaggle/working/nn_models/fold{fi}/* /tmp/workdir/kaggle/working/nn_models/fold{fi}/')\n        \n            # Avoid some errors\n            with K.name_scope(CFG.optimizer.__class__.__name__):\n                for j, var in enumerate(CFG.optimizer.weights):\n                    name = 'variable{}'.format(j)\n                    CFG.optimizer.weights[j] = tf.Variable(var, name=name)\n            CFG.conf = CFG.conf._replace(optimizer=CFG.optimizer)\n\n            oof_preds = nn_model.predict(X.iloc[valid_idx], verbose=1, batch_size=512).flatten()\n            rmse = np.round(np.sqrt(np.mean((oof_preds - y.iloc[valid_idx])**2)),4)\n            print(f'{Fore.GREEN}{Style.BRIGHT}\\nFold {fi+1} | rmse: {rmse}\\n')\n            if fi<CFG.folds: oof[valid_idx] = oof_preds\n            else: oof[valid_idx] += oof_preds\n                \n        elif CFG.nn == False and CFG.ctb == True:\n            print('\\n',\"ctb only model training.\",'\\n')\n            X_ = X.copy()\n            if CFG.scaler is not None:\n                print(f'\\nInverse scaling {len(num_cols)} numerical cols for ctb.')\n                X_[num_cols] = scaler.inverse_transform(X_[num_cols])\n            ctb_model = CatBoostRegressor(**CFG.ctb_params)\n            ctb_model.fit(X_.iloc[train_idx], y.iloc[train_idx],\n                          eval_set=[(X_.iloc[valid_idx], y.iloc[valid_idx])],\n                          cat_features=cat_cols, use_best_model=True)\n            ctb_models.append(ctb_model)\n            \n            ctb_model.save_model(f'/kaggle/working/ctb_models/fold{fi}/ctb_model.cbm')\n\n            oof_preds = ctb_model.predict(X_.iloc[valid_idx])\n            rmse = np.round(np.sqrt(np.mean((oof_preds - y.iloc[valid_idx])**2)),4)\n            print(f'\\nFold {fi+1} | rmse: {rmse}\\n')\n            if fi<CFG.folds: oof[valid_idx] = oof_preds\n            else: oof[valid_idx] += oof_preds\n                \n        elif CFG.nn == True and CFG.ctb == True:\n            print('\\n',\"nn & ctb model training.\",'\\n')\n            K.clear_session()\n            nn_model = DeepTable(config=CFG.conf)\n            nn_model.fit(X.iloc[train_idx], y.iloc[train_idx],\n                      validation_data=(X.iloc[valid_idx], y.iloc[valid_idx]),\n                      callbacks=CFG.LR_Scheduler,\n                      batch_size=CFG.batch_size, epochs=CFG.epochs, verbose=2)\n            nn_models.append(nn_model)\n            \n            # Save model\n            nn_model.save(f'/kaggle/working/nn_models/fold{fi}')\n            os.system(f'cp -r /kaggle/working/nn_models/fold{fi}/* /tmp/workdir/kaggle/working/nn_models/fold{fi}/')\n\n            # Avoid some errors\n            with K.name_scope(CFG.optimizer.__class__.__name__):\n                for j, var in enumerate(CFG.optimizer.weights):\n                    name = 'variable{}'.format(j)\n                    CFG.optimizer.weights[j] = tf.Variable(var, name=name)\n            CFG.conf = CFG.conf._replace(optimizer=CFG.optimizer)\n            \n            X_ = X.copy()\n            if CFG.scaler is not None:\n                print(f'\\nInverse scaling {len(num_cols)} numerical cols for ctb.')\n                X_[num_cols] = scaler.inverse_transform(X_[num_cols])\n            ctb_model = CatBoostRegressor(**CFG.ctb_params)\n            ctb_model.fit(X_.iloc[train_idx], y.iloc[train_idx],\n                          eval_set=[(X_.iloc[valid_idx], y.iloc[valid_idx])],\n                          cat_features=cat_cols, use_best_model=True)\n            ctb_models.append(ctb_model)\n            \n            ctb_model.save_model(f'/kaggle/working/ctb_models/fold{fi}/ctb_model.cbm')\n\n            oof_preds = CFG.ens_weights['nn'] * nn_model.predict(X.iloc[valid_idx],\n                                                                 verbose=1, batch_size=512).flatten() + \\\n                        CFG.ens_weights['ctb'] * ctb_model.predict(X_.iloc[valid_idx])\n            rmse = np.round(np.sqrt(np.mean((oof_preds - y.iloc[valid_idx])**2)),4)\n            print(f'{Fore.GREEN}{Style.BRIGHT}\\nFold {fi+1} | rmse: {rmse}\\n')\n            if fi<CFG.folds: oof[valid_idx] = oof_preds\n            else: oof[valid_idx] += oof_preds\n                \n        else:\n            raise ValueError(\"No model selected in CFG.\")\n    \n    rmse = np.round(np.sqrt(np.mean((oof - y)**2)),4)\n    print(f'{Fore.BLUE}{Style.BRIGHT}Overall CV rmse: {rmse}\\n')\n    if CFG.nn==True: plot_model(nn_model.get_model().model)\n    return nn_models, ctb_models\n\n\ndef infer(data, nn_models, ctb_models, num_cols, scaler):\n    if CFG.nn == True and CFG.ctb == False:\n        return np.mean([model.predict(data, verbose=1, batch_size=512).flatten()\n                                            for model in nn_models], axis=0)\n    elif CFG.nn == False and CFG.ctb == True:\n        if CFG.scaler is not None:\n            print(f'Inverse scaling {len(num_cols)} numerical cols for ctb.\\n')\n            data[num_cols] = scaler.inverse_transform(data[num_cols])\n        return np.mean([model.predict(data) for model in ctb_models], axis=0)\n    \n    elif CFG.nn == True and CFG.ctb == True:\n        data_ = data.copy()\n        if CFG.scaler is not None:\n            print(f'Inverse scaling {len(num_cols)} numerical cols for ctb.\\n')\n            data_[num_cols] = scaler.inverse_transform(data_[num_cols])\n        return CFG.ens_weights['nn'] * np.mean([model.predict(data, verbose=1, batch_size=512).flatten()\n                                                for model in nn_models], axis=0) + \\\n               CFG.ens_weights['ctb'] * np.mean([model.predict(data_) for model in ctb_models], axis=0)\n    else:\n        raise ValueError(\"No model selected in CFG.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T16:58:05.889410Z","iopub.execute_input":"2024-11-28T16:58:05.889813Z","iopub.status.idle":"2024-11-28T16:58:05.913650Z","shell.execute_reply.started":"2024-11-28T16:58:05.889772Z","shell.execute_reply":"2024-11-28T16:58:05.912845Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"%%time\nrun_i = 0\nscaler = CFG.scaler\ndef predict(test_data, submission):\n    global run_i, scaler, nn_models, ctb_models\n    if run_i == 0:\n        train_df = pl.read_csv(CFG.train_path)\n        train_df, cat_cols, num_cols = preprocess_data(train_df)\n        if scaler is not None:\n            print(f'Scaling {len(num_cols)} numerical cols.\\n')\n            train_df[num_cols] = scaler.fit_transform(train_df[num_cols])\n        nn_models, ctb_models = train(train_df, cat_cols, num_cols, scaler)\n    run_i += 1\n    test_df, cat_cols, num_cols = preprocess_data(test_data)\n    test_df = test_df.drop(columns=game_col)\n    if scaler is not None:\n        print(f'Scaling {len(num_cols)} numerical cols.\\n')\n        test_df[num_cols] = scaler.transform(test_df[num_cols])\n    return submission.with_columns(pl.Series(target_col, infer(test_df, nn_models, ctb_models,\n                                                               num_cols, scaler)))\n\ninference_server = kaggle_evaluation.mcts_inference_server.MCTSInferenceServer(predict)\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(\n        ('/kaggle/input/um-game-playing-strength-of-mcts-variants/test.csv',\n         '/kaggle/input/um-game-playing-strength-of-mcts-variants/sample_submission.csv'))","metadata":{"_kg_hide-output":true,"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T16:58:05.914684Z","iopub.execute_input":"2024-11-28T16:58:05.914948Z","iopub.status.idle":"2024-11-28T17:32:50.411297Z","shell.execute_reply.started":"2024-11-28T16:58:05.914924Z","shell.execute_reply":"2024-11-28T17:32:50.410168Z"}},"outputs":[{"name":"stdout","text":"Data shape: (233234, 598)\n\nScaling 588 numerical cols.\n\nnn = True\nctb = True \n\n#########################\n### Fold 1/6 ...\n#########################\n\n nn & ctb model training. \n\n11-28 16:58:17 I deeptables.m.deeptable.py 338 - X.Shape=(194328, 596), y.Shape=(194328,), batch_size=128, config=ModelConfig(name='conf-1', nets=['cin_nets', 'dnn_nets', 'fm_nets'], categorical_columns='auto', exclude_columns=[], task='regression', pos_label=None, metrics=['RootMeanSquaredError'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=False, auto_scale=False, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=True, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.2, stacking_op='concat', output_use_bias=False, apply_class_weight=False, optimizer=<keras.src.optimizers.legacy.adam.Adam object at 0x7f43749b02b0>, loss='auto', dnn_params={'hidden_units': ((1024, 0.0, True), (512, 0.0, True), (256, 0.0, True), (128, 0.0, True)), 'dnn_activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=1, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n11-28 16:58:17 I deeptables.m.deeptable.py 339 - metrics:['RootMeanSquaredError']\n","output_type":"stream"},{"name":"stderr","text":"11-28 16:58:19 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n    cache_key = tb.data_hasher()(key_items)\n  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n    for x in self._iter_data(data):\n  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n    yield from self._iter_data(v)\n  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n    yield from self._iter_data(x)\n  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n\n","output_type":"stream"},{"name":"stdout","text":"11-28 16:58:19 I deeptables.m.preprocessor.py 263 - Preparing features...\n11-28 16:58:20 I deeptables.m.preprocessor.py 338 - Preparing features taken 0.6535210609436035s\n11-28 16:58:20 I deeptables.m.preprocessor.py 390 - Categorical encoding...\n11-28 16:58:20 I deeptables.m.preprocessor.py 395 - Categorical encoding taken 0.22066378593444824s\n11-28 16:58:20 I deeptables.m.preprocessor.py 436 - Extracting GBM features...\n11-28 16:58:21 I hypernets.t.sklearn_ex.py 640 - LightGBM task:regression\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.385300 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 17137\n[LightGBM] [Info] Number of data points in the train set: 194328, number of used features: 593\n[LightGBM] [Info] Start training from score 0.048944\n11-28 16:58:34 I deeptables.m.preprocessor.py 447 - Extracting gbm features taken 13.947038173675537s\n11-28 16:58:35 I deeptables.m.preprocessor.py 198 - fit_transform taken 16.30700135231018s\n11-28 16:58:35 I deeptables.m.preprocessor.py 244 - Transform [X]...\n11-28 16:58:36 I deeptables.m.preprocessor.py 251 - transform_X taken 0.804816484451294s\n11-28 16:58:36 I deeptables.m.preprocessor.py 232 - Transform [y]...\n11-28 16:58:36 I deeptables.m.preprocessor.py 238 - transform_y taken 0.00043582916259765625s\n11-28 16:58:36 I deeptables.m.deeptable.py 354 - Training...\n11-28 16:58:36 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_rootmeansquarederror, patience:1, mode:min\n2 Physical GPUs, 2 Logical GPUs\n11-28 16:58:36 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n11-28 16:58:40 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n11-28 16:58:40 I deeptables.m.deepmodel.py 235 - Building model...\n11-28 16:58:42 I deeptables.m.deepmodel.py 291 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n---------------------------------------------------------\ninputs:\n---------------------------------------------------------\n['all_categorical_vars: (108)', 'input_continuous_all: (585)']\n---------------------------------------------------------\nembeddings:\n---------------------------------------------------------\ninput_dims: [6, 5, 5, 4, 6, 5, 5, 4, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31]\noutput_dims: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\ndropout: 0.2\n---------------------------------------------------------\ndense: dropout: 0\nbatch_normalization: False\n---------------------------------------------------------\nconcat_embed_dense: shape: (None, 1017)\n---------------------------------------------------------\nnets: ['cin_nets', 'dnn_nets', 'fm_nets']\n---------------------------------------------------------\ncin: input_shape (None, 108, 4), output_shape (None, 1)\ndnn: input_shape (None, 1017), output_shape (None, 128)\nfm: input_shape (None, 108, 4), output_shape (None, 1)\n---------------------------------------------------------\nstacking_op: concat\n---------------------------------------------------------\noutput: activation: None, output_shape: (None, 1), use_bias: False\nloss: mse\noptimizer: Adam\n---------------------------------------------------------\n\n11-28 16:58:42 I deeptables.m.deepmodel.py 105 - training...\n\nEpoch 1: LearningRateScheduler setting learning rate to 0.0001.\nEpoch 1/7\n1518/1518 - 50s - loss: 0.1924 - root_mean_squared_error: 0.4387 - val_loss: 0.2376 - val_root_mean_squared_error: 0.4875 - lr: 1.0000e-04 - 50s/epoch - 33ms/step\n\nEpoch 2: LearningRateScheduler setting learning rate to 0.001.\nEpoch 2/7\n1518/1518 - 30s - loss: 0.1014 - root_mean_squared_error: 0.3184 - val_loss: 0.1970 - val_root_mean_squared_error: 0.4438 - lr: 0.0010 - 30s/epoch - 20ms/step\n\nEpoch 3: LearningRateScheduler setting learning rate to 0.001.\nEpoch 3/7\n1518/1518 - 30s - loss: 0.0781 - root_mean_squared_error: 0.2794 - val_loss: 0.1908 - val_root_mean_squared_error: 0.4368 - lr: 0.0010 - 30s/epoch - 20ms/step\n\nEpoch 4: LearningRateScheduler setting learning rate to 0.001.\nEpoch 4/7\n1518/1518 - 30s - loss: 0.0718 - root_mean_squared_error: 0.2680 - val_loss: 0.1980 - val_root_mean_squared_error: 0.4449 - lr: 0.0010 - 30s/epoch - 20ms/step\n\nEpoch 5: LearningRateScheduler setting learning rate to 0.001.\nEpoch 5/7\n1518/1518 - 30s - loss: 0.0673 - root_mean_squared_error: 0.2594 - val_loss: 0.1991 - val_root_mean_squared_error: 0.4463 - lr: 0.0010 - 30s/epoch - 20ms/step\n\nEpoch 6: LearningRateScheduler setting learning rate to 0.001.\nEpoch 6/7\n1518/1518 - 30s - loss: 0.0637 - root_mean_squared_error: 0.2523 - val_loss: 0.1929 - val_root_mean_squared_error: 0.4392 - lr: 0.0010 - 30s/epoch - 20ms/step\n\nEpoch 7: LearningRateScheduler setting learning rate to 0.001.\nEpoch 7/7\n1518/1518 - 30s - loss: 0.0602 - root_mean_squared_error: 0.2453 - val_loss: 0.1921 - val_root_mean_squared_error: 0.4383 - lr: 0.0010 - 30s/epoch - 20ms/step\n11-28 17:02:34 I deeptables.m.deepmodel.py 122 - Training finished.\n11-28 17:02:34 I deeptables.m.deeptable.py 370 - Training finished.\n\nInverse scaling 588 numerical cols for ctb.\n","output_type":"stream"},{"name":"stderr","text":"Warning: less than 75% GPU memory available for training. Free: 1024.125 Total: 15095.0625\n","output_type":"stream"},{"name":"stdout","text":"0:\tlearn: 0.6025957\ttest: 0.6229694\tbest: 0.6229694 (0)\ttotal: 188ms\tremaining: 6m 15s\n500:\tlearn: 0.2568627\ttest: 0.4245008\tbest: 0.4245008 (500)\ttotal: 21.7s\tremaining: 1m 4s\n1000:\tlearn: 0.2150377\ttest: 0.4171259\tbest: 0.4171259 (1000)\ttotal: 43.9s\tremaining: 43.8s\n1500:\tlearn: 0.1890452\ttest: 0.4151588\tbest: 0.4151551 (1499)\ttotal: 1m 6s\tremaining: 22.1s\nbestTest = 0.4150153501\nbestIteration = 1631\nShrink model to first 1632 iterations.\n11-28 17:03:59 I deeptables.m.deeptable.py 818 - Load model from: /kaggle/working/nn_models/fold0/cin_nets+dnn_nets+fm_nets.h5.\n2 Physical GPUs, 2 Logical GPUs\n11-28 17:04:01 I deeptables.m.deeptable.py 685 - Perform prediction...\n11-28 17:04:01 I deeptables.m.preprocessor.py 244 - Transform [X]...\n11-28 17:04:02 I deeptables.m.preprocessor.py 251 - transform_X taken 1.1715643405914307s\n11-28 17:04:02 I deeptables.m.deepmodel.py 130 - Performing predictions...\n11-28 17:04:02 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=False\n76/76 [==============================] - 3s 19ms/step\n11-28 17:04:05 I deeptables.m.deeptable.py 559 - predict_proba taken 5.801797866821289s\n\u001b[32m\u001b[1m\nFold 1 | rmse: 0.4135\n\n#########################\n### Fold 2/6 ...\n#########################\n\n nn & ctb model training. \n\n11-28 17:04:06 I deeptables.m.deeptable.py 338 - X.Shape=(194394, 596), y.Shape=(194394,), batch_size=128, config=ModelConfig(name='conf-1', nets=['cin_nets', 'dnn_nets', 'fm_nets'], categorical_columns='auto', exclude_columns=[], task='regression', pos_label=None, metrics=['RootMeanSquaredError'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=False, auto_scale=False, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=True, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.2, stacking_op='concat', output_use_bias=False, apply_class_weight=False, optimizer=<keras.src.optimizers.legacy.adam.Adam object at 0x7f43749b02b0>, loss='auto', dnn_params={'hidden_units': ((1024, 0.0, True), (512, 0.0, True), (256, 0.0, True), (128, 0.0, True)), 'dnn_activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=1, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n11-28 17:04:06 I deeptables.m.deeptable.py 339 - metrics:['RootMeanSquaredError']\n","output_type":"stream"},{"name":"stderr","text":"11-28 17:04:07 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n    cache_key = tb.data_hasher()(key_items)\n  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n    for x in self._iter_data(data):\n  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n    yield from self._iter_data(v)\n  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n    yield from self._iter_data(x)\n  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n\n","output_type":"stream"},{"name":"stdout","text":"11-28 17:04:08 I deeptables.m.preprocessor.py 263 - Preparing features...\n11-28 17:04:08 I deeptables.m.preprocessor.py 338 - Preparing features taken 0.628685474395752s\n11-28 17:04:08 I deeptables.m.preprocessor.py 390 - Categorical encoding...\n11-28 17:04:09 I deeptables.m.preprocessor.py 395 - Categorical encoding taken 0.21700310707092285s\n11-28 17:04:09 I deeptables.m.preprocessor.py 436 - Extracting GBM features...\n11-28 17:04:09 I hypernets.t.sklearn_ex.py 640 - LightGBM task:regression\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.283375 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 17176\n[LightGBM] [Info] Number of data points in the train set: 194394, number of used features: 593\n[LightGBM] [Info] Start training from score 0.049932\n11-28 17:04:22 I deeptables.m.preprocessor.py 447 - Extracting gbm features taken 13.292171001434326s\n11-28 17:04:23 I deeptables.m.preprocessor.py 198 - fit_transform taken 15.614793062210083s\n11-28 17:04:23 I deeptables.m.preprocessor.py 244 - Transform [X]...\n11-28 17:04:24 I deeptables.m.preprocessor.py 251 - transform_X taken 0.7837886810302734s\n11-28 17:04:24 I deeptables.m.preprocessor.py 232 - Transform [y]...\n11-28 17:04:24 I deeptables.m.preprocessor.py 238 - transform_y taken 0.0004088878631591797s\n11-28 17:04:24 I deeptables.m.deeptable.py 354 - Training...\n2 Physical GPUs, 2 Logical GPUs\n11-28 17:04:24 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n11-28 17:04:27 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n11-28 17:04:27 I deeptables.m.deepmodel.py 235 - Building model...\n11-28 17:04:28 I deeptables.m.deepmodel.py 291 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n---------------------------------------------------------\ninputs:\n---------------------------------------------------------\n['all_categorical_vars: (108)', 'input_continuous_all: (585)']\n---------------------------------------------------------\nembeddings:\n---------------------------------------------------------\ninput_dims: [6, 5, 5, 4, 6, 5, 5, 4, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31]\noutput_dims: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\ndropout: 0.2\n---------------------------------------------------------\ndense: dropout: 0\nbatch_normalization: False\n---------------------------------------------------------\nconcat_embed_dense: shape: (None, 1017)\n---------------------------------------------------------\nnets: ['cin_nets', 'dnn_nets', 'fm_nets']\n---------------------------------------------------------\ncin: input_shape (None, 108, 4), output_shape (None, 1)\ndnn: input_shape (None, 1017), output_shape (None, 128)\nfm: input_shape (None, 108, 4), output_shape (None, 1)\n---------------------------------------------------------\nstacking_op: concat\n---------------------------------------------------------\noutput: activation: None, output_shape: (None, 1), use_bias: False\nloss: mse\noptimizer: Adam\n---------------------------------------------------------\n\n11-28 17:04:28 I deeptables.m.deepmodel.py 105 - training...\n\nEpoch 1: LearningRateScheduler setting learning rate to 0.0001.\nEpoch 1/7\n1518/1518 - 46s - loss: 0.1340 - root_mean_squared_error: 0.3661 - val_loss: 0.2252 - val_root_mean_squared_error: 0.4746 - lr: 1.0000e-04 - 46s/epoch - 30ms/step\n\nEpoch 2: LearningRateScheduler setting learning rate to 0.001.\nEpoch 2/7\n1518/1518 - 31s - loss: 0.0961 - root_mean_squared_error: 0.3099 - val_loss: 0.2064 - val_root_mean_squared_error: 0.4543 - lr: 0.0010 - 31s/epoch - 20ms/step\n\nEpoch 3: LearningRateScheduler setting learning rate to 0.001.\nEpoch 3/7\n1518/1518 - 31s - loss: 0.0769 - root_mean_squared_error: 0.2773 - val_loss: 0.2099 - val_root_mean_squared_error: 0.4582 - lr: 0.0010 - 31s/epoch - 20ms/step\n\nEpoch 4: LearningRateScheduler setting learning rate to 0.001.\nEpoch 4/7\n1518/1518 - 31s - loss: 0.0705 - root_mean_squared_error: 0.2655 - val_loss: 0.2081 - val_root_mean_squared_error: 0.4562 - lr: 0.0010 - 31s/epoch - 20ms/step\n\nEpoch 5: LearningRateScheduler setting learning rate to 0.001.\nEpoch 5/7\n1518/1518 - 31s - loss: 0.0660 - root_mean_squared_error: 0.2569 - val_loss: 0.2108 - val_root_mean_squared_error: 0.4592 - lr: 0.0010 - 31s/epoch - 20ms/step\n\nEpoch 6: LearningRateScheduler setting learning rate to 0.001.\nEpoch 6/7\n1518/1518 - 32s - loss: 0.0622 - root_mean_squared_error: 0.2495 - val_loss: 0.2131 - val_root_mean_squared_error: 0.4617 - lr: 0.0010 - 32s/epoch - 21ms/step\n\nEpoch 7: LearningRateScheduler setting learning rate to 0.001.\nEpoch 7/7\n1518/1518 - 32s - loss: 0.0587 - root_mean_squared_error: 0.2422 - val_loss: 0.2073 - val_root_mean_squared_error: 0.4553 - lr: 0.0010 - 32s/epoch - 21ms/step\n11-28 17:08:22 I deeptables.m.deepmodel.py 122 - Training finished.\n11-28 17:08:22 I deeptables.m.deeptable.py 370 - Training finished.\n\nInverse scaling 588 numerical cols for ctb.\n","output_type":"stream"},{"name":"stderr","text":"Warning: less than 75% GPU memory available for training. Free: 1022.125 Total: 15095.0625\n","output_type":"stream"},{"name":"stdout","text":"0:\tlearn: 0.6055009\ttest: 0.6198111\tbest: 0.6198111 (0)\ttotal: 65.2ms\tremaining: 2m 10s\n500:\tlearn: 0.2602212\ttest: 0.4379919\tbest: 0.4379919 (500)\ttotal: 22.1s\tremaining: 1m 6s\n1000:\tlearn: 0.2193575\ttest: 0.4342196\tbest: 0.4341869 (988)\ttotal: 44.7s\tremaining: 44.6s\nbestTest = 0.4334631016\nbestIteration = 1207\nShrink model to first 1208 iterations.\n11-28 17:09:29 I deeptables.m.deeptable.py 818 - Load model from: /kaggle/working/nn_models/fold1/cin_nets+dnn_nets+fm_nets.h5.\n2 Physical GPUs, 2 Logical GPUs\n11-28 17:09:31 I deeptables.m.deeptable.py 685 - Perform prediction...\n11-28 17:09:31 I deeptables.m.preprocessor.py 244 - Transform [X]...\n11-28 17:09:32 I deeptables.m.preprocessor.py 251 - transform_X taken 0.8071844577789307s\n11-28 17:09:32 I deeptables.m.deepmodel.py 130 - Performing predictions...\n11-28 17:09:32 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=False\n76/76 [==============================] - 2s 20ms/step\n11-28 17:09:34 I deeptables.m.deeptable.py 559 - predict_proba taken 4.890743017196655s\n\u001b[32m\u001b[1m\nFold 2 | rmse: 0.4303\n\n#########################\n### Fold 3/6 ...\n#########################\n\n nn & ctb model training. \n\n11-28 17:09:35 I deeptables.m.deeptable.py 338 - X.Shape=(194328, 596), y.Shape=(194328,), batch_size=128, config=ModelConfig(name='conf-1', nets=['cin_nets', 'dnn_nets', 'fm_nets'], categorical_columns='auto', exclude_columns=[], task='regression', pos_label=None, metrics=['RootMeanSquaredError'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=False, auto_scale=False, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=True, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.2, stacking_op='concat', output_use_bias=False, apply_class_weight=False, optimizer=<keras.src.optimizers.legacy.adam.Adam object at 0x7f43749b02b0>, loss='auto', dnn_params={'hidden_units': ((1024, 0.0, True), (512, 0.0, True), (256, 0.0, True), (128, 0.0, True)), 'dnn_activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=1, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n11-28 17:09:35 I deeptables.m.deeptable.py 339 - metrics:['RootMeanSquaredError']\n","output_type":"stream"},{"name":"stderr","text":"11-28 17:09:37 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n    cache_key = tb.data_hasher()(key_items)\n  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n    for x in self._iter_data(data):\n  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n    yield from self._iter_data(v)\n  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n    yield from self._iter_data(x)\n  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n\n","output_type":"stream"},{"name":"stdout","text":"11-28 17:09:37 I deeptables.m.preprocessor.py 263 - Preparing features...\n11-28 17:09:38 I deeptables.m.preprocessor.py 338 - Preparing features taken 0.6574482917785645s\n11-28 17:09:38 I deeptables.m.preprocessor.py 390 - Categorical encoding...\n11-28 17:09:38 I deeptables.m.preprocessor.py 395 - Categorical encoding taken 0.22511720657348633s\n11-28 17:09:38 I deeptables.m.preprocessor.py 436 - Extracting GBM features...\n11-28 17:09:39 I hypernets.t.sklearn_ex.py 640 - LightGBM task:regression\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.286650 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 17212\n[LightGBM] [Info] Number of data points in the train set: 194328, number of used features: 594\n[LightGBM] [Info] Start training from score 0.044576\n11-28 17:09:51 I deeptables.m.preprocessor.py 447 - Extracting gbm features taken 12.695677518844604s\n11-28 17:09:52 I deeptables.m.preprocessor.py 198 - fit_transform taken 15.061270713806152s\n11-28 17:09:52 I deeptables.m.preprocessor.py 244 - Transform [X]...\n11-28 17:09:53 I deeptables.m.preprocessor.py 251 - transform_X taken 0.8032243251800537s\n11-28 17:09:53 I deeptables.m.preprocessor.py 232 - Transform [y]...\n11-28 17:09:53 I deeptables.m.preprocessor.py 238 - transform_y taken 0.0007779598236083984s\n11-28 17:09:53 I deeptables.m.deeptable.py 354 - Training...\n2 Physical GPUs, 2 Logical GPUs\n11-28 17:09:53 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n11-28 17:09:56 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n11-28 17:09:56 I deeptables.m.deepmodel.py 235 - Building model...\n11-28 17:09:57 I deeptables.m.deepmodel.py 291 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n---------------------------------------------------------\ninputs:\n---------------------------------------------------------\n['all_categorical_vars: (108)', 'input_continuous_all: (586)']\n---------------------------------------------------------\nembeddings:\n---------------------------------------------------------\ninput_dims: [6, 5, 5, 4, 6, 5, 5, 4, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31]\noutput_dims: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\ndropout: 0.2\n---------------------------------------------------------\ndense: dropout: 0\nbatch_normalization: False\n---------------------------------------------------------\nconcat_embed_dense: shape: (None, 1018)\n---------------------------------------------------------\nnets: ['cin_nets', 'dnn_nets', 'fm_nets']\n---------------------------------------------------------\ncin: input_shape (None, 108, 4), output_shape (None, 1)\ndnn: input_shape (None, 1018), output_shape (None, 128)\nfm: input_shape (None, 108, 4), output_shape (None, 1)\n---------------------------------------------------------\nstacking_op: concat\n---------------------------------------------------------\noutput: activation: None, output_shape: (None, 1), use_bias: False\nloss: mse\noptimizer: Adam\n---------------------------------------------------------\n\n11-28 17:09:57 I deeptables.m.deepmodel.py 105 - training...\n\nEpoch 1: LearningRateScheduler setting learning rate to 0.0001.\nEpoch 1/7\n1518/1518 - 47s - loss: 0.1575 - root_mean_squared_error: 0.3969 - val_loss: 0.2021 - val_root_mean_squared_error: 0.4495 - lr: 1.0000e-04 - 47s/epoch - 31ms/step\n\nEpoch 2: LearningRateScheduler setting learning rate to 0.001.\nEpoch 2/7\n1518/1518 - 31s - loss: 0.1005 - root_mean_squared_error: 0.3170 - val_loss: 0.1800 - val_root_mean_squared_error: 0.4243 - lr: 0.0010 - 31s/epoch - 21ms/step\n\nEpoch 3: LearningRateScheduler setting learning rate to 0.001.\nEpoch 3/7\n1518/1518 - 31s - loss: 0.0786 - root_mean_squared_error: 0.2804 - val_loss: 0.1704 - val_root_mean_squared_error: 0.4127 - lr: 0.0010 - 31s/epoch - 21ms/step\n\nEpoch 4: LearningRateScheduler setting learning rate to 0.001.\nEpoch 4/7\n1518/1518 - 31s - loss: 0.0720 - root_mean_squared_error: 0.2684 - val_loss: 0.1772 - val_root_mean_squared_error: 0.4209 - lr: 0.0010 - 31s/epoch - 20ms/step\n\nEpoch 5: LearningRateScheduler setting learning rate to 0.001.\nEpoch 5/7\n1518/1518 - 31s - loss: 0.0672 - root_mean_squared_error: 0.2592 - val_loss: 0.1723 - val_root_mean_squared_error: 0.4151 - lr: 0.0010 - 31s/epoch - 20ms/step\n\nEpoch 6: LearningRateScheduler setting learning rate to 0.001.\nEpoch 6/7\n1518/1518 - 31s - loss: 0.0632 - root_mean_squared_error: 0.2514 - val_loss: 0.1702 - val_root_mean_squared_error: 0.4126 - lr: 0.0010 - 31s/epoch - 20ms/step\n\nEpoch 7: LearningRateScheduler setting learning rate to 0.001.\nEpoch 7/7\n1518/1518 - 31s - loss: 0.0598 - root_mean_squared_error: 0.2446 - val_loss: 0.1703 - val_root_mean_squared_error: 0.4127 - lr: 0.0010 - 31s/epoch - 20ms/step\n11-28 17:13:50 I deeptables.m.deepmodel.py 122 - Training finished.\n11-28 17:13:51 I deeptables.m.deeptable.py 370 - Training finished.\n\nInverse scaling 588 numerical cols for ctb.\n","output_type":"stream"},{"name":"stderr","text":"Warning: less than 75% GPU memory available for training. Free: 1022.125 Total: 15095.0625\n","output_type":"stream"},{"name":"stdout","text":"0:\tlearn: 0.6083648\ttest: 0.5998373\tbest: 0.5998373 (0)\ttotal: 66ms\tremaining: 2m 11s\n500:\tlearn: 0.2559170\ttest: 0.4169104\tbest: 0.4169104 (500)\ttotal: 22s\tremaining: 1m 5s\n1000:\tlearn: 0.2143153\ttest: 0.4125558\tbest: 0.4125325 (996)\ttotal: 44s\tremaining: 43.9s\n1500:\tlearn: 0.1889409\ttest: 0.4113204\tbest: 0.4112065 (1487)\ttotal: 1m 6s\tremaining: 22.1s\nbestTest = 0.4108991458\nbestIteration = 1826\nShrink model to first 1827 iterations.\n11-28 17:15:26 I deeptables.m.deeptable.py 818 - Load model from: /kaggle/working/nn_models/fold2/cin_nets+dnn_nets+fm_nets.h5.\n2 Physical GPUs, 2 Logical GPUs\n11-28 17:15:27 I deeptables.m.deeptable.py 685 - Perform prediction...\n11-28 17:15:27 I deeptables.m.preprocessor.py 244 - Transform [X]...\n11-28 17:15:29 I deeptables.m.preprocessor.py 251 - transform_X taken 1.2120380401611328s\n11-28 17:15:29 I deeptables.m.deepmodel.py 130 - Performing predictions...\n11-28 17:15:29 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=False\n76/76 [==============================] - 2s 14ms/step\n11-28 17:15:31 I deeptables.m.deeptable.py 559 - predict_proba taken 4.948981761932373s\n\u001b[32m\u001b[1m\nFold 3 | rmse: 0.3991\n\n#########################\n### Fold 4/6 ...\n#########################\n\n nn & ctb model training. \n\n11-28 17:15:32 I deeptables.m.deeptable.py 338 - X.Shape=(194372, 596), y.Shape=(194372,), batch_size=128, config=ModelConfig(name='conf-1', nets=['cin_nets', 'dnn_nets', 'fm_nets'], categorical_columns='auto', exclude_columns=[], task='regression', pos_label=None, metrics=['RootMeanSquaredError'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=False, auto_scale=False, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=True, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.2, stacking_op='concat', output_use_bias=False, apply_class_weight=False, optimizer=<keras.src.optimizers.legacy.adam.Adam object at 0x7f43749b02b0>, loss='auto', dnn_params={'hidden_units': ((1024, 0.0, True), (512, 0.0, True), (256, 0.0, True), (128, 0.0, True)), 'dnn_activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=1, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n11-28 17:15:32 I deeptables.m.deeptable.py 339 - metrics:['RootMeanSquaredError']\n","output_type":"stream"},{"name":"stderr","text":"11-28 17:15:34 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n    cache_key = tb.data_hasher()(key_items)\n  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n    for x in self._iter_data(data):\n  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n    yield from self._iter_data(v)\n  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n    yield from self._iter_data(x)\n  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n\n","output_type":"stream"},{"name":"stdout","text":"11-28 17:15:34 I deeptables.m.preprocessor.py 263 - Preparing features...\n11-28 17:15:35 I deeptables.m.preprocessor.py 338 - Preparing features taken 0.6381473541259766s\n11-28 17:15:35 I deeptables.m.preprocessor.py 390 - Categorical encoding...\n11-28 17:15:35 I deeptables.m.preprocessor.py 395 - Categorical encoding taken 0.21989679336547852s\n11-28 17:15:35 I deeptables.m.preprocessor.py 436 - Extracting GBM features...\n11-28 17:15:36 I hypernets.t.sklearn_ex.py 640 - LightGBM task:regression\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.283955 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 17149\n[LightGBM] [Info] Number of data points in the train set: 194372, number of used features: 583\n[LightGBM] [Info] Start training from score 0.038129\n11-28 17:15:48 I deeptables.m.preprocessor.py 447 - Extracting gbm features taken 13.00431203842163s\n11-28 17:15:49 I deeptables.m.preprocessor.py 198 - fit_transform taken 15.535844087600708s\n11-28 17:15:49 I deeptables.m.preprocessor.py 244 - Transform [X]...\n11-28 17:15:50 I deeptables.m.preprocessor.py 251 - transform_X taken 0.8125107288360596s\n11-28 17:15:50 I deeptables.m.preprocessor.py 232 - Transform [y]...\n11-28 17:15:50 I deeptables.m.preprocessor.py 238 - transform_y taken 0.0004513263702392578s\n11-28 17:15:51 I deeptables.m.deeptable.py 354 - Training...\n2 Physical GPUs, 2 Logical GPUs\n11-28 17:15:51 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n11-28 17:15:53 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n11-28 17:15:53 I deeptables.m.deepmodel.py 235 - Building model...\n11-28 17:15:54 I deeptables.m.deepmodel.py 291 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n---------------------------------------------------------\ninputs:\n---------------------------------------------------------\n['all_categorical_vars: (108)', 'input_continuous_all: (575)']\n---------------------------------------------------------\nembeddings:\n---------------------------------------------------------\ninput_dims: [6, 5, 5, 4, 6, 5, 5, 4, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31]\noutput_dims: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\ndropout: 0.2\n---------------------------------------------------------\ndense: dropout: 0\nbatch_normalization: False\n---------------------------------------------------------\nconcat_embed_dense: shape: (None, 1007)\n---------------------------------------------------------\nnets: ['cin_nets', 'dnn_nets', 'fm_nets']\n---------------------------------------------------------\ncin: input_shape (None, 108, 4), output_shape (None, 1)\ndnn: input_shape (None, 1007), output_shape (None, 128)\nfm: input_shape (None, 108, 4), output_shape (None, 1)\n---------------------------------------------------------\nstacking_op: concat\n---------------------------------------------------------\noutput: activation: None, output_shape: (None, 1), use_bias: False\nloss: mse\noptimizer: Adam\n---------------------------------------------------------\n\n11-28 17:15:54 I deeptables.m.deepmodel.py 105 - training...\n\nEpoch 1: LearningRateScheduler setting learning rate to 0.0001.\nEpoch 1/7\n1518/1518 - 47s - loss: 0.1245 - root_mean_squared_error: 0.3529 - val_loss: 0.2016 - val_root_mean_squared_error: 0.4490 - lr: 1.0000e-04 - 47s/epoch - 31ms/step\n\nEpoch 2: LearningRateScheduler setting learning rate to 0.001.\nEpoch 2/7\n1518/1518 - 31s - loss: 0.0932 - root_mean_squared_error: 0.3053 - val_loss: 0.1950 - val_root_mean_squared_error: 0.4416 - lr: 0.0010 - 31s/epoch - 21ms/step\n\nEpoch 3: LearningRateScheduler setting learning rate to 0.001.\nEpoch 3/7\n1518/1518 - 31s - loss: 0.0766 - root_mean_squared_error: 0.2768 - val_loss: 0.1926 - val_root_mean_squared_error: 0.4389 - lr: 0.0010 - 31s/epoch - 21ms/step\n\nEpoch 4: LearningRateScheduler setting learning rate to 0.001.\nEpoch 4/7\n1518/1518 - 31s - loss: 0.0705 - root_mean_squared_error: 0.2655 - val_loss: 0.1930 - val_root_mean_squared_error: 0.4393 - lr: 0.0010 - 31s/epoch - 21ms/step\n\nEpoch 5: LearningRateScheduler setting learning rate to 0.001.\nEpoch 5/7\n1518/1518 - 31s - loss: 0.0652 - root_mean_squared_error: 0.2553 - val_loss: 0.1955 - val_root_mean_squared_error: 0.4422 - lr: 0.0010 - 31s/epoch - 21ms/step\n\nEpoch 6: LearningRateScheduler setting learning rate to 0.001.\nEpoch 6/7\n1518/1518 - 31s - loss: 0.0612 - root_mean_squared_error: 0.2474 - val_loss: 0.1977 - val_root_mean_squared_error: 0.4447 - lr: 0.0010 - 31s/epoch - 21ms/step\n\nEpoch 7: LearningRateScheduler setting learning rate to 0.001.\nEpoch 7/7\n1518/1518 - 31s - loss: 0.0578 - root_mean_squared_error: 0.2403 - val_loss: 0.1948 - val_root_mean_squared_error: 0.4414 - lr: 0.0010 - 31s/epoch - 21ms/step\n11-28 17:19:50 I deeptables.m.deepmodel.py 122 - Training finished.\n11-28 17:19:50 I deeptables.m.deeptable.py 370 - Training finished.\n\nInverse scaling 588 numerical cols for ctb.\n","output_type":"stream"},{"name":"stderr","text":"Warning: less than 75% GPU memory available for training. Free: 1022.125 Total: 15095.0625\n","output_type":"stream"},{"name":"stdout","text":"0:\tlearn: 0.6052111\ttest: 0.6113171\tbest: 0.6113171 (0)\ttotal: 65.8ms\tremaining: 2m 11s\n500:\tlearn: 0.2572432\ttest: 0.4308310\tbest: 0.4308310 (500)\ttotal: 22.4s\tremaining: 1m 6s\n1000:\tlearn: 0.2159358\ttest: 0.4257324\tbest: 0.4257271 (996)\ttotal: 44.5s\tremaining: 44.4s\n1500:\tlearn: 0.1896634\ttest: 0.4241897\tbest: 0.4241767 (1494)\ttotal: 1m 7s\tremaining: 22.4s\nbestTest = 0.4237403681\nbestIteration = 1717\nShrink model to first 1718 iterations.\n11-28 17:21:22 I deeptables.m.deeptable.py 818 - Load model from: /kaggle/working/nn_models/fold3/cin_nets+dnn_nets+fm_nets.h5.\n2 Physical GPUs, 2 Logical GPUs\n11-28 17:21:24 I deeptables.m.deeptable.py 685 - Perform prediction...\n11-28 17:21:24 I deeptables.m.preprocessor.py 244 - Transform [X]...\n11-28 17:21:25 I deeptables.m.preprocessor.py 251 - transform_X taken 0.7916877269744873s\n11-28 17:21:25 I deeptables.m.deepmodel.py 130 - Performing predictions...\n11-28 17:21:25 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=False\n76/76 [==============================] - 2s 20ms/step\n11-28 17:21:27 I deeptables.m.deeptable.py 559 - predict_proba taken 4.958720684051514s\n\u001b[32m\u001b[1m\nFold 4 | rmse: 0.4174\n\n#########################\n### Fold 5/6 ...\n#########################\n\n nn & ctb model training. \n\n11-28 17:21:28 I deeptables.m.deeptable.py 338 - X.Shape=(194374, 596), y.Shape=(194374,), batch_size=128, config=ModelConfig(name='conf-1', nets=['cin_nets', 'dnn_nets', 'fm_nets'], categorical_columns='auto', exclude_columns=[], task='regression', pos_label=None, metrics=['RootMeanSquaredError'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=False, auto_scale=False, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=True, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.2, stacking_op='concat', output_use_bias=False, apply_class_weight=False, optimizer=<keras.src.optimizers.legacy.adam.Adam object at 0x7f43749b02b0>, loss='auto', dnn_params={'hidden_units': ((1024, 0.0, True), (512, 0.0, True), (256, 0.0, True), (128, 0.0, True)), 'dnn_activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=1, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n11-28 17:21:28 I deeptables.m.deeptable.py 339 - metrics:['RootMeanSquaredError']\n","output_type":"stream"},{"name":"stderr","text":"11-28 17:21:30 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n    cache_key = tb.data_hasher()(key_items)\n  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n    for x in self._iter_data(data):\n  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n    yield from self._iter_data(v)\n  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n    yield from self._iter_data(x)\n  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n\n","output_type":"stream"},{"name":"stdout","text":"11-28 17:21:31 I deeptables.m.preprocessor.py 263 - Preparing features...\n11-28 17:21:31 I deeptables.m.preprocessor.py 338 - Preparing features taken 0.6360223293304443s\n11-28 17:21:31 I deeptables.m.preprocessor.py 390 - Categorical encoding...\n11-28 17:21:32 I deeptables.m.preprocessor.py 395 - Categorical encoding taken 0.22440862655639648s\n11-28 17:21:32 I deeptables.m.preprocessor.py 436 - Extracting GBM features...\n11-28 17:21:33 I hypernets.t.sklearn_ex.py 640 - LightGBM task:regression\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.290361 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 16991\n[LightGBM] [Info] Number of data points in the train set: 194374, number of used features: 595\n[LightGBM] [Info] Start training from score 0.031522\n11-28 17:21:46 I deeptables.m.preprocessor.py 447 - Extracting gbm features taken 13.836369514465332s\n11-28 17:21:47 I deeptables.m.preprocessor.py 198 - fit_transform taken 16.441981315612793s\n11-28 17:21:47 I deeptables.m.preprocessor.py 244 - Transform [X]...\n11-28 17:21:48 I deeptables.m.preprocessor.py 251 - transform_X taken 0.7931654453277588s\n11-28 17:21:48 I deeptables.m.preprocessor.py 232 - Transform [y]...\n11-28 17:21:48 I deeptables.m.preprocessor.py 238 - transform_y taken 0.0005171298980712891s\n11-28 17:21:48 I deeptables.m.deeptable.py 354 - Training...\n2 Physical GPUs, 2 Logical GPUs\n11-28 17:21:48 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n11-28 17:21:51 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n11-28 17:21:52 I deeptables.m.deepmodel.py 235 - Building model...\n11-28 17:21:53 I deeptables.m.deepmodel.py 291 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n---------------------------------------------------------\ninputs:\n---------------------------------------------------------\n['all_categorical_vars: (108)', 'input_continuous_all: (587)']\n---------------------------------------------------------\nembeddings:\n---------------------------------------------------------\ninput_dims: [6, 5, 5, 4, 6, 5, 5, 4, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31]\noutput_dims: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\ndropout: 0.2\n---------------------------------------------------------\ndense: dropout: 0\nbatch_normalization: False\n---------------------------------------------------------\nconcat_embed_dense: shape: (None, 1019)\n---------------------------------------------------------\nnets: ['cin_nets', 'dnn_nets', 'fm_nets']\n---------------------------------------------------------\ncin: input_shape (None, 108, 4), output_shape (None, 1)\ndnn: input_shape (None, 1019), output_shape (None, 128)\nfm: input_shape (None, 108, 4), output_shape (None, 1)\n---------------------------------------------------------\nstacking_op: concat\n---------------------------------------------------------\noutput: activation: None, output_shape: (None, 1), use_bias: False\nloss: mse\noptimizer: Adam\n---------------------------------------------------------\n\n11-28 17:21:53 I deeptables.m.deepmodel.py 105 - training...\n\nEpoch 1: LearningRateScheduler setting learning rate to 0.0001.\nEpoch 1/7\n1518/1518 - 46s - loss: 0.1098 - root_mean_squared_error: 0.3314 - val_loss: 0.1920 - val_root_mean_squared_error: 0.4382 - lr: 1.0000e-04 - 46s/epoch - 30ms/step\n\nEpoch 2: LearningRateScheduler setting learning rate to 0.001.\nEpoch 2/7\n1518/1518 - 31s - loss: 0.0900 - root_mean_squared_error: 0.3001 - val_loss: 0.1915 - val_root_mean_squared_error: 0.4377 - lr: 0.0010 - 31s/epoch - 20ms/step\n\nEpoch 3: LearningRateScheduler setting learning rate to 0.001.\nEpoch 3/7\n1518/1518 - 31s - loss: 0.0748 - root_mean_squared_error: 0.2734 - val_loss: 0.1878 - val_root_mean_squared_error: 0.4333 - lr: 0.0010 - 31s/epoch - 20ms/step\n\nEpoch 4: LearningRateScheduler setting learning rate to 0.001.\nEpoch 4/7\n1518/1518 - 31s - loss: 0.0679 - root_mean_squared_error: 0.2607 - val_loss: 0.1905 - val_root_mean_squared_error: 0.4364 - lr: 0.0010 - 31s/epoch - 20ms/step\n\nEpoch 5: LearningRateScheduler setting learning rate to 0.001.\nEpoch 5/7\n1518/1518 - 31s - loss: 0.0629 - root_mean_squared_error: 0.2508 - val_loss: 0.1918 - val_root_mean_squared_error: 0.4379 - lr: 0.0010 - 31s/epoch - 20ms/step\n\nEpoch 6: LearningRateScheduler setting learning rate to 0.001.\nEpoch 6/7\n1518/1518 - 31s - loss: 0.0589 - root_mean_squared_error: 0.2427 - val_loss: 0.1910 - val_root_mean_squared_error: 0.4370 - lr: 0.0010 - 31s/epoch - 20ms/step\n\nEpoch 7: LearningRateScheduler setting learning rate to 0.001.\nEpoch 7/7\n1518/1518 - 31s - loss: 0.0555 - root_mean_squared_error: 0.2356 - val_loss: 0.1869 - val_root_mean_squared_error: 0.4323 - lr: 0.0010 - 31s/epoch - 20ms/step\n11-28 17:25:44 I deeptables.m.deepmodel.py 122 - Training finished.\n11-28 17:25:44 I deeptables.m.deeptable.py 370 - Training finished.\n\nInverse scaling 588 numerical cols for ctb.\n","output_type":"stream"},{"name":"stderr","text":"Warning: less than 75% GPU memory available for training. Free: 1022.125 Total: 15095.0625\n","output_type":"stream"},{"name":"stdout","text":"0:\tlearn: 0.6071787\ttest: 0.6118390\tbest: 0.6118390 (0)\ttotal: 65.2ms\tremaining: 2m 10s\n500:\tlearn: 0.2559892\ttest: 0.4273470\tbest: 0.4273470 (500)\ttotal: 22s\tremaining: 1m 5s\n1000:\tlearn: 0.2154791\ttest: 0.4238437\tbest: 0.4238328 (990)\ttotal: 45.1s\tremaining: 45.1s\nbestTest = 0.4234228363\nbestIteration = 1061\nShrink model to first 1062 iterations.\n11-28 17:26:48 I deeptables.m.deeptable.py 818 - Load model from: /kaggle/working/nn_models/fold4/cin_nets+dnn_nets+fm_nets.h5.\n2 Physical GPUs, 2 Logical GPUs\n11-28 17:26:49 I deeptables.m.deeptable.py 685 - Perform prediction...\n11-28 17:26:49 I deeptables.m.preprocessor.py 244 - Transform [X]...\n11-28 17:26:51 I deeptables.m.preprocessor.py 251 - transform_X taken 1.637458324432373s\n11-28 17:26:51 I deeptables.m.deepmodel.py 130 - Performing predictions...\n11-28 17:26:51 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=False\n76/76 [==============================] - 2s 20ms/step\n11-28 17:26:54 I deeptables.m.deeptable.py 559 - predict_proba taken 5.989092826843262s\n\u001b[32m\u001b[1m\nFold 5 | rmse: 0.4152\n\n#########################\n### Fold 6/6 ...\n#########################\n\n nn & ctb model training. \n\n11-28 17:26:55 I deeptables.m.deeptable.py 338 - X.Shape=(194374, 596), y.Shape=(194374,), batch_size=128, config=ModelConfig(name='conf-1', nets=['cin_nets', 'dnn_nets', 'fm_nets'], categorical_columns='auto', exclude_columns=[], task='regression', pos_label=None, metrics=['RootMeanSquaredError'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=False, auto_scale=False, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=True, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.2, stacking_op='concat', output_use_bias=False, apply_class_weight=False, optimizer=<keras.src.optimizers.legacy.adam.Adam object at 0x7f43749b02b0>, loss='auto', dnn_params={'hidden_units': ((1024, 0.0, True), (512, 0.0, True), (256, 0.0, True), (128, 0.0, True)), 'dnn_activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=1, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n11-28 17:26:55 I deeptables.m.deeptable.py 339 - metrics:['RootMeanSquaredError']\n","output_type":"stream"},{"name":"stderr","text":"11-28 17:26:57 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n    cache_key = tb.data_hasher()(key_items)\n  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n    for x in self._iter_data(data):\n  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n    yield from self._iter_data(v)\n  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n    yield from self._iter_data(x)\n  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n\n","output_type":"stream"},{"name":"stdout","text":"11-28 17:26:58 I deeptables.m.preprocessor.py 263 - Preparing features...\n11-28 17:26:58 I deeptables.m.preprocessor.py 338 - Preparing features taken 0.6426620483398438s\n11-28 17:26:58 I deeptables.m.preprocessor.py 390 - Categorical encoding...\n11-28 17:26:58 I deeptables.m.preprocessor.py 395 - Categorical encoding taken 0.21929311752319336s\n11-28 17:26:58 I deeptables.m.preprocessor.py 436 - Extracting GBM features...\n11-28 17:27:00 I hypernets.t.sklearn_ex.py 640 - LightGBM task:regression\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.280954 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 17062\n[LightGBM] [Info] Number of data points in the train set: 194374, number of used features: 587\n[LightGBM] [Info] Start training from score 0.053284\n11-28 17:27:13 I deeptables.m.preprocessor.py 447 - Extracting gbm features taken 14.9073805809021s\n11-28 17:27:15 I deeptables.m.preprocessor.py 198 - fit_transform taken 17.55028796195984s\n11-28 17:27:15 I deeptables.m.preprocessor.py 244 - Transform [X]...\n11-28 17:27:16 I deeptables.m.preprocessor.py 251 - transform_X taken 0.8069005012512207s\n11-28 17:27:16 I deeptables.m.preprocessor.py 232 - Transform [y]...\n11-28 17:27:16 I deeptables.m.preprocessor.py 238 - transform_y taken 0.0005257129669189453s\n11-28 17:27:16 I deeptables.m.deeptable.py 354 - Training...\n2 Physical GPUs, 2 Logical GPUs\n11-28 17:27:16 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n11-28 17:27:18 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n11-28 17:27:19 I deeptables.m.deepmodel.py 235 - Building model...\n11-28 17:27:20 I deeptables.m.deepmodel.py 291 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n---------------------------------------------------------\ninputs:\n---------------------------------------------------------\n['all_categorical_vars: (108)', 'input_continuous_all: (579)']\n---------------------------------------------------------\nembeddings:\n---------------------------------------------------------\ninput_dims: [6, 5, 5, 4, 6, 5, 5, 4, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31]\noutput_dims: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\ndropout: 0.2\n---------------------------------------------------------\ndense: dropout: 0\nbatch_normalization: False\n---------------------------------------------------------\nconcat_embed_dense: shape: (None, 1011)\n---------------------------------------------------------\nnets: ['cin_nets', 'dnn_nets', 'fm_nets']\n---------------------------------------------------------\ncin: input_shape (None, 108, 4), output_shape (None, 1)\ndnn: input_shape (None, 1011), output_shape (None, 128)\nfm: input_shape (None, 108, 4), output_shape (None, 1)\n---------------------------------------------------------\nstacking_op: concat\n---------------------------------------------------------\noutput: activation: None, output_shape: (None, 1), use_bias: False\nloss: mse\noptimizer: Adam\n---------------------------------------------------------\n\n11-28 17:27:20 I deeptables.m.deepmodel.py 105 - training...\n\nEpoch 1: LearningRateScheduler setting learning rate to 0.0001.\nEpoch 1/7\n1518/1518 - 47s - loss: 0.1298 - root_mean_squared_error: 0.3602 - val_loss: 0.1929 - val_root_mean_squared_error: 0.4392 - lr: 1.0000e-04 - 47s/epoch - 31ms/step\n\nEpoch 2: LearningRateScheduler setting learning rate to 0.001.\nEpoch 2/7\n1518/1518 - 32s - loss: 0.0943 - root_mean_squared_error: 0.3071 - val_loss: 0.1824 - val_root_mean_squared_error: 0.4271 - lr: 0.0010 - 32s/epoch - 21ms/step\n\nEpoch 3: LearningRateScheduler setting learning rate to 0.001.\nEpoch 3/7\n1518/1518 - 31s - loss: 0.0764 - root_mean_squared_error: 0.2764 - val_loss: 0.1917 - val_root_mean_squared_error: 0.4379 - lr: 0.0010 - 31s/epoch - 21ms/step\n\nEpoch 4: LearningRateScheduler setting learning rate to 0.001.\nEpoch 4/7\n1518/1518 - 32s - loss: 0.0704 - root_mean_squared_error: 0.2654 - val_loss: 0.1852 - val_root_mean_squared_error: 0.4304 - lr: 0.0010 - 32s/epoch - 21ms/step\n\nEpoch 5: LearningRateScheduler setting learning rate to 0.001.\nEpoch 5/7\n1518/1518 - 32s - loss: 0.0649 - root_mean_squared_error: 0.2548 - val_loss: 0.1831 - val_root_mean_squared_error: 0.4279 - lr: 0.0010 - 32s/epoch - 21ms/step\n\nEpoch 6: LearningRateScheduler setting learning rate to 0.001.\nEpoch 6/7\n1518/1518 - 32s - loss: 0.0611 - root_mean_squared_error: 0.2471 - val_loss: 0.1832 - val_root_mean_squared_error: 0.4281 - lr: 0.0010 - 32s/epoch - 21ms/step\n\nEpoch 7: LearningRateScheduler setting learning rate to 0.001.\nEpoch 7/7\n1518/1518 - 32s - loss: 0.0576 - root_mean_squared_error: 0.2400 - val_loss: 0.1864 - val_root_mean_squared_error: 0.4317 - lr: 0.0010 - 32s/epoch - 21ms/step\n11-28 17:31:17 I deeptables.m.deepmodel.py 122 - Training finished.\n11-28 17:31:17 I deeptables.m.deeptable.py 370 - Training finished.\n\nInverse scaling 588 numerical cols for ctb.\n","output_type":"stream"},{"name":"stderr","text":"Warning: less than 75% GPU memory available for training. Free: 1022.125 Total: 15095.0625\n","output_type":"stream"},{"name":"stdout","text":"0:\tlearn: 0.6058592\ttest: 0.6066060\tbest: 0.6066060 (0)\ttotal: 65.7ms\tremaining: 2m 11s\n500:\tlearn: 0.2541632\ttest: 0.4249519\tbest: 0.4249519 (500)\ttotal: 22.3s\tremaining: 1m 6s\n1000:\tlearn: 0.2138783\ttest: 0.4225612\tbest: 0.4225612 (1000)\ttotal: 44.9s\tremaining: 44.8s\n1500:\tlearn: 0.1892022\ttest: 0.4220002\tbest: 0.4219849 (1496)\ttotal: 1m 7s\tremaining: 22.5s\nbestTest = 0.4219804372\nbestIteration = 1521\nShrink model to first 1522 iterations.\n11-28 17:32:42 I deeptables.m.deeptable.py 818 - Load model from: /kaggle/working/nn_models/fold5/cin_nets+dnn_nets+fm_nets.h5.\n2 Physical GPUs, 2 Logical GPUs\n11-28 17:32:44 I deeptables.m.deeptable.py 685 - Perform prediction...\n11-28 17:32:44 I deeptables.m.preprocessor.py 244 - Transform [X]...\n11-28 17:32:45 I deeptables.m.preprocessor.py 251 - transform_X taken 0.78395676612854s\n11-28 17:32:45 I deeptables.m.deepmodel.py 130 - Performing predictions...\n11-28 17:32:45 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=False\n76/76 [==============================] - 2s 14ms/step\n11-28 17:32:47 I deeptables.m.deeptable.py 559 - predict_proba taken 4.596129417419434s\n\u001b[32m\u001b[1m\nFold 6 | rmse: 0.4128\n\n\u001b[34m\u001b[1mOverall CV rmse: 0.4148\n\nData shape: (3, 597)\n\nScaling 588 numerical cols.\n\nInverse scaling 588 numerical cols for ctb.\n\n11-28 17:32:48 I deeptables.m.deeptable.py 685 - Perform prediction...\n11-28 17:32:48 I deeptables.m.preprocessor.py 244 - Transform [X]...\n11-28 17:32:48 I deeptables.m.preprocessor.py 251 - transform_X taken 0.08936834335327148s\n11-28 17:32:48 I deeptables.m.deepmodel.py 130 - Performing predictions...\n11-28 17:32:48 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=False\n1/1 [==============================] - 0s 293ms/step\n11-28 17:32:48 I deeptables.m.deeptable.py 559 - predict_proba taken 0.40409088134765625s\n11-28 17:32:48 I deeptables.m.deeptable.py 685 - Perform prediction...\n11-28 17:32:48 I deeptables.m.preprocessor.py 244 - Transform [X]...\n11-28 17:32:49 I deeptables.m.preprocessor.py 251 - transform_X taken 0.08996343612670898s\n11-28 17:32:49 I deeptables.m.deepmodel.py 130 - Performing predictions...\n11-28 17:32:49 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=False\n1/1 [==============================] - 0s 194ms/step\n11-28 17:32:49 I deeptables.m.deeptable.py 559 - predict_proba taken 0.3008897304534912s\n11-28 17:32:49 I deeptables.m.deeptable.py 685 - Perform prediction...\n11-28 17:32:49 I deeptables.m.preprocessor.py 244 - Transform [X]...\n11-28 17:32:49 I deeptables.m.preprocessor.py 251 - transform_X taken 0.09210562705993652s\n11-28 17:32:49 I deeptables.m.deepmodel.py 130 - Performing predictions...\n11-28 17:32:49 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=False\n1/1 [==============================] - 0s 200ms/step\n11-28 17:32:49 I deeptables.m.deeptable.py 559 - predict_proba taken 0.30989623069763184s\n11-28 17:32:49 I deeptables.m.deeptable.py 685 - Perform prediction...\n11-28 17:32:49 I deeptables.m.preprocessor.py 244 - Transform [X]...\n11-28 17:32:49 I deeptables.m.preprocessor.py 251 - transform_X taken 0.0922386646270752s\n11-28 17:32:49 I deeptables.m.deepmodel.py 130 - Performing predictions...\n11-28 17:32:49 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=False\n1/1 [==============================] - 0s 209ms/step\n11-28 17:32:49 I deeptables.m.deeptable.py 559 - predict_proba taken 0.32173681259155273s\n11-28 17:32:49 I deeptables.m.deeptable.py 685 - Perform prediction...\n11-28 17:32:49 I deeptables.m.preprocessor.py 244 - Transform [X]...\n11-28 17:32:49 I deeptables.m.preprocessor.py 251 - transform_X taken 0.09026956558227539s\n11-28 17:32:49 I deeptables.m.deepmodel.py 130 - Performing predictions...\n11-28 17:32:49 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=False\n1/1 [==============================] - 0s 199ms/step\n11-28 17:32:50 I deeptables.m.deeptable.py 559 - predict_proba taken 0.306286096572876s\n11-28 17:32:50 I deeptables.m.deeptable.py 685 - Perform prediction...\n11-28 17:32:50 I deeptables.m.preprocessor.py 244 - Transform [X]...\n11-28 17:32:50 I deeptables.m.preprocessor.py 251 - transform_X taken 0.09203410148620605s\n11-28 17:32:50 I deeptables.m.deepmodel.py 130 - Performing predictions...\n11-28 17:32:50 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=False\n1/1 [==============================] - 0s 12ms/step\n11-28 17:32:50 I deeptables.m.deeptable.py 559 - predict_proba taken 0.12508344650268555s\nCPU times: user 1h 2min 17s, sys: 7min 50s, total: 1h 10min 7s\nWall time: 34min 44s\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Check load model\ndef load_model(paths):\n    models = []\n    for fold in sorted(os.listdir(paths)):\n        path = os.path.join(paths, fold)\n        for file in os.listdir(path):\n            if file.endswith('.h5'):\n                models.append(DeepTable.load(path, file))\n            elif file.endswith('.cbm'):\n                print('Load model from:', path+'/'+file)\n                models.append(CatBoostRegressor().load_model(path+'/'+file))\n    return models\n\nnn_models = load_model(\"/kaggle/working/nn_models\")\nprint('nn_models:', nn_models,'\\n')\n\nctb_models = load_model(\"/kaggle/working/ctb_models\")\nprint('ctb_models:', ctb_models)","metadata":{"_kg_hide-output":true,"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T17:32:50.412621Z","iopub.execute_input":"2024-11-28T17:32:50.412971Z","iopub.status.idle":"2024-11-28T17:33:04.652555Z","shell.execute_reply.started":"2024-11-28T17:32:50.412943Z","shell.execute_reply":"2024-11-28T17:33:04.651597Z"}},"outputs":[{"name":"stdout","text":"11-28 17:32:50 I deeptables.m.deeptable.py 818 - Load model from: /kaggle/working/nn_models/fold0/cin_nets+dnn_nets+fm_nets.h5.\n2 Physical GPUs, 2 Logical GPUs\n11-28 17:32:53 I deeptables.m.deeptable.py 818 - Load model from: /kaggle/working/nn_models/fold1/cin_nets+dnn_nets+fm_nets.h5.\n2 Physical GPUs, 2 Logical GPUs\n11-28 17:32:54 I deeptables.m.deeptable.py 818 - Load model from: /kaggle/working/nn_models/fold2/cin_nets+dnn_nets+fm_nets.h5.\n2 Physical GPUs, 2 Logical GPUs\n11-28 17:32:57 I deeptables.m.deeptable.py 818 - Load model from: /kaggle/working/nn_models/fold3/cin_nets+dnn_nets+fm_nets.h5.\n2 Physical GPUs, 2 Logical GPUs\n11-28 17:33:00 I deeptables.m.deeptable.py 818 - Load model from: /kaggle/working/nn_models/fold4/cin_nets+dnn_nets+fm_nets.h5.\n2 Physical GPUs, 2 Logical GPUs\n11-28 17:33:03 I deeptables.m.deeptable.py 818 - Load model from: /kaggle/working/nn_models/fold5/cin_nets+dnn_nets+fm_nets.h5.\n2 Physical GPUs, 2 Logical GPUs\nnn_models: [<deeptables.models.deeptable.DeepTable object at 0x7f40efa5c400>, <deeptables.models.deeptable.DeepTable object at 0x7f40f0242200>, <deeptables.models.deeptable.DeepTable object at 0x7f40f055c730>, <deeptables.models.deeptable.DeepTable object at 0x7f40ebf62ef0>, <deeptables.models.deeptable.DeepTable object at 0x7f40ec2bbdf0>, <deeptables.models.deeptable.DeepTable object at 0x7f40e9667220>] \n\nLoad model from: /kaggle/working/ctb_models/fold0/ctb_model.cbm\nLoad model from: /kaggle/working/ctb_models/fold1/ctb_model.cbm\nLoad model from: /kaggle/working/ctb_models/fold2/ctb_model.cbm\nLoad model from: /kaggle/working/ctb_models/fold3/ctb_model.cbm\nLoad model from: /kaggle/working/ctb_models/fold4/ctb_model.cbm\nLoad model from: /kaggle/working/ctb_models/fold5/ctb_model.cbm\nctb_models: [<catboost.core.CatBoostRegressor object at 0x7f43447e7520>, <catboost.core.CatBoostRegressor object at 0x7f43447e7010>, <catboost.core.CatBoostRegressor object at 0x7f43447e7d60>, <catboost.core.CatBoostRegressor object at 0x7f445e249c00>, <catboost.core.CatBoostRegressor object at 0x7f42b1aefaf0>, <catboost.core.CatBoostRegressor object at 0x7f42b1aec3a0>]\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}